\documentclass[11pt]{article}
\pagestyle{plain}
%\documentclass{article}
%\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}

\usepackage{latexsym,amsmath,amssymb}
\usepackage{amsthm}
%\usepackage[notref,notcite]{showkeys}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{thmtools}
\usepackage{wrapfig}
\usepackage{extarrows}
\usepackage{breqn}
\usepackage{physics}
\usepackage{afterpage}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{mathrsfs}
\usepackage{scalerel}
\usepackage{stackengine,wasysym}
\usepackage{aligned-overset}
\usepackage{stackengine}
\usepackage{mathtools}
\usepackage{nccmath}
\usepackage{url}
\graphicspath{ {images/} }

\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = blue,
    filecolor = magenta,      
    urlcolor = blue,
    citecolor = blue,
}

\urlstyle{same}


\setlength{\oddsidemargin}{1pt}
\setlength{\evensidemargin}{1pt}
\setlength{\marginparwidth}{30pt} % these gain 53pt width
\setlength{\topmargin}{1pt}       % gains 26pt height
\setlength{\headheight}{1pt}      % gains 11pt height
\setlength{\headsep}{1pt}         % gains 24pt height
%\setlength{\footheight}{12 pt} 	  % cannot be changed as number must fit
\setlength{\footskip}{24pt}       % gains 6pt height
\setlength{\textheight}{650pt}    % 528 + 26 + 11 + 24 + 6 + 55 for luck
\setlength{\textwidth}{460pt}     % 360 + 53 + 47 for luck

\title{Sections and Chapters}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{remark}{Remark}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\numberwithin{equation}{subsection}

\def\dsp{\def\baselinestretch{1.35}\large
\normalsize}
%%%%This makes a double spacing. Use this with 11pt style. If you
%%%%want to use this just insert \dsp after the \begin{document}
%%%%The correct baselinestretch for double spacing is 1.37. However
%%%%you can use different parameter.


\def\U{{\mathcal U}}

\begin{document}

\centerline{\Large \bf Algebra Qualifying Exam}
\centerline{Zhen Yao}

\bigskip

\section{May 2019 Exam}

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item Prove or give a counterexample: if $T: \mathbb{R}^3 \to \mathbb{R}^3$ is a linear transformation such that $N_T \cap R_T$ has dimension at least $1$, then $T$ is nilpotent. 
    
    \item Prove or give a counterexample: if $T: \mathbb{R}^4 \to \mathbb{R}^4$ is a linear transformation such that $N_T \cap R_T$ has dimension at least $2$, then $T$ is nilpotent. 
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Consider $T(x,y,z) = (0,x,z)$\cite{1}. Then it is obvious that $T$ is linear. Also, $N_T = \{(0,y,0)| y \in \mathbb{R}\}$ and $R_T = \{(0,x,y)| x,y \in \mathbb{R}\}$. Then, $N_T \cap R_T = \{(0,y,0)| y \in \mathbb{R}\}$, which is at least dimension $1$. However, $T$ is not nilpotent, since 
    \begin{align*}
        T^3(x,y,z) = T^2(0,x,z) = T(0,0,z) = (0,0,z).
    \end{align*}
    
    \item With the rank-nullity theorem, we have 
    \begin{align*}
        \dim N_T + \dim R_T = \dim \mathbb{R}^4 = 4,
    \end{align*}
    also, 
    \begin{align*}
        \dim (N_T + R_T) = \dim N_T + \dim R_T - \dim (N_T \cap R_T) \leq 2.
    \end{align*}
    Then the only possibility is that $\dim N_T = \dim R_T = 2$ and $\dim (N_T \cap R_T) = 2$, which implies $R_T = N_T$\cite{2}. Thus, $T^2(x) = T(T(x)) = 0$, since $T(x) \in R_T$. Hence, $T$ is nilpotent.
\end{enumerate}
\end{proof}

We present another approach to prove part $(b)$.
\begin{proof}
Let $u,v \in N_T \cap R_T$ such that $u, v$ are linearly independent. Then, $Tu = 0$ and $Tv = 0$, and for some $u', v' \in \mathbb{R}^4$, $u = Tu'$ and $v = Tv'$. Define $S = \{u, v, u', v'\}$ and each element of $S$ is annihilated by some power of $T$, saying $T^2$. 

We claim $u, v, u', v'$ are linearly independent. Suppose $c_1 u + c_2 v + c_3 u' + c_4 v' = 0$. We need to prove that $c_i = 0, i = 1,2,3,4$. Applying $T$ to the above equation gives
\begin{align*}
    c_1 T u + c_2 T v + c_3 T u' + c_4 T v' = 0,
\end{align*}
which implies
\begin{align*}
    c_3 u + c_4 v = 0
\end{align*}
and then $c_3 = c_4 = 0$. Thus, $c_1 = c_2 = 0$. Hence, $\{u, v, u', v'\}$ is also nilpotent.
\end{proof}

\medskip

\begin{exercise}
Suppose that $\lambda$ is an eigenvalue of a matrix $A \in \mathbb{C}^{n \times n}$ with algebraic multiplicity $k$. Show that $(A - \lambda I)^k$ has rank $n - k$.
\end{exercise}
\begin{proof}
$A$ is similar to its Jordan Canonial form $J_A(\lambda)$, such that there exists an invertible matrix $S$ such that $A = S J_A S^{-1}$. Suppose under such $S$, $J_A$ has form
\begin{align*}
    J_A = \begin{pmatrix}
        J(\lambda) & 0 & \cdots & 0 \\
        0 & J(\lambda_1) & \cdots & 0 \\
        \vdots & \ddots & \ddots & \vdots \\
        0 & \cdots & \cdots & J(\lambda_l)
    \end{pmatrix},
\end{align*}
where $\lambda, \lambda_1, \cdots, \lambda_l$ are $l+1$ eigenvalues of $A$ and
\begin{align*}
    J(\lambda) = \begin{pmatrix}
        \lambda & 1 & \cdots & \cdots & 0 \\
        0 & \lambda & \cdots & \cdots &  0 \\
        \vdots & \ddots & \ddots & \vdots & 1 \\
        0 & \cdots & \cdots & \cdots & \lambda
    \end{pmatrix}_{k \times k}.
\end{align*}
Also, $(A - \lambda I)^k = S (J_A - \lambda I)^k S^{-1}$, and 
\begin{align*}
    (J(\lambda) - \lambda I) = \begin{pmatrix}
        0 & 1 & \cdots & \cdots & 0 \\
        0 & 0 & \cdots & \cdots &  0 \\
        \vdots & \ddots & \ddots & \vdots & 1 \\
        0 & \cdots & \cdots & \cdots & 0
    \end{pmatrix}_{k \times k}, (J(\lambda) - \lambda I)^k = \begin{pmatrix}
        0 & 0 & \cdots & \cdots & 0 \\
        0 & 0 & \cdots & \cdots &  0 \\
        \vdots & \ddots & \ddots & \vdots & 0 \\
        0 & \cdots & \cdots & \cdots & 0
    \end{pmatrix}_{k \times k}.
\end{align*}
Also, $(J(\lambda_i) - \lambda I)^k \neq 0$ for $i = 1, \cdots, l$, then we have $\dim N_{(J_A - \lambda I)^k} = k$ since there is a $k\times k$ zero matrix in $J_A$. Thus, $\dim R_{(J_A - \lambda I)^k} = n - k$. Since $A$ and $J_A(\lambda)$ are similar, then they has the same eigenvalues and furthermore, $\dim \dim N_{(A - \lambda I)^k} = \dim N_{(J_A - \lambda I)^k}$. The rest follows.
\end{proof}

\medskip

\begin{exercise}{\rm *}
For any $n \geq 1$, classify the matrices $Q \in \mathbb{R}^{n \times n}$ that are both orthogonal and skew-symmetric, meaning $Q^T = -Q$, up to similarity, i.e., exhibit exactly one representative from each real similarity class. \\
{\bf Hint:} The answer is very different for odd versus even $n$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For $n$ is odd, we have $Q^T  = -Q$ and $Q^T Q = I$, Then, $- Q^2 = I$, and then $\det (- Q^2) = 1$. Thus, $(-1)^n \det (Q^2) = 1$. And there is no such matrix.
    
    \item For $n = 2k, k \in \mathbb{N}$ is even, we have the same equaitons as before and $Q^2 + I = 0$. Then the minimal polynomial $m_Q$ of $Q$ satisfies $m_Q(x)|(x^2 + 1)$. 
    
    Also, we claim all eigenvalues of $Q$ are non-zero purely imaginary\cite{3}. Indeed, for $x$ being eigenvector of $Q$ corresponding to eigenvalue $\lambda$, we have 
    \begin{align*}
        \lambda \|x\|^2 = (Ax, x) = (x, A^T x) = (x, - A x) = - \overline{\lambda} \|x\|^2,
    \end{align*}
    thus, $\lambda = -\overline{\lambda}$, which implies $\lambda$ is purely imaginary.
    
    Then, eigenvalues of $Q$ are $i$ and $-i$ since $m_Q(x)|(x^2 + 1)$. Also, $Q$ is diagonalizable\footnote{For every normal matrix $A$, there is a unitary matrix $P$ such that $PAP^{-1}$ is a diagonal matrix. ({\em Linear Algebra}. K. Hoffman  and R.A. Kunze. P317)} in $\mathbb{C}$, since $Q^T Q = Q Q^T = I$, then $Q$ is a normal map and can be unitary diagonalized\cite{4}. Let $x_1, y_1 \in \mathbb{C}^n$ be linear independent, such that for eigenvalue $i$ and $-i$, we have \footnote{Let $Q$ be an orthogonal map on $\mathbb{R}^n$ and suppose $\lambda$ is nonzero eigenvalue of $Q$. Then there exist linearly independent $x,y \in \mathbb{R}^n$ such that for $\lambda = \alpha + i \beta$, we have $Qx = \alpha x - \beta y, Qy = \beta x - \alpha y$.} 
    \begin{align*}
        Q x_1 = y_1, Q y_1 = - x_1.
    \end{align*}
    Let $W_1 = {\rm span}\{x_1, y_1\}$ and $\dim W_1 = 2$. Also, $Q(W_1) \subset W$, and then we have $Q(W_1^\bot) \subset W_1^\bot$ \footnote{Let $Q$ be an orthogonal map on $\mathbb{R}^n$. Let $W \subset \mathbb{R}^n$ such that $Q(W) \subset W$ or $Q(W) = W$, then $Q(W^\bot) \subset W$ or $Q(W^\bot) = W$.}. Thus, $Q|_{W_1^\bot}$ is orthogonal and skew-symmetric since 
    \begin{align*}
        Q|_{W_1^\bot} = \begin{pmatrix}
            \left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right] & 0 \\
            0 & Q_1
        \end{pmatrix}.
    \end{align*}
    Then pick $x_2, y_2 \in \mathbb{C}^n$ be linear independent, such that 
    \begin{align*}
        Q x_2 = y_2, Q y_2 = - x_2,
    \end{align*}
    and $W_2 = {\rm span}\{x_2, y_2\}$. Then, $W_1^\bot = W_2 \oplus W_2^\bot$. Continue this process for $k$ times and we have $\mathbb{R}^n = W_1 \oplus W_2 \cdots \oplus W_k$, where $W_i = {\rm span}\{x_i, y_i\}$ and 
    \begin{align*}
        Q x_i = y_i, Q y_i = - x_i.
    \end{align*}
    Now, let $B = {\rm span}\{x_1, y_1, \cdots, x_k, y_k\}$, then $B$ is a basis of $\mathbb{R}^n$ and 
    \begin{align*}
        [Q]_B = 
        \begin{pmatrix}
            \begin{array}{cccc}
                \left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right] &  \\
                 & \left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right]
            \end{array} & 0 \\
            0 & \begin{array}{cccc}
                \ddots &  \\
                 & \left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right]
            \end{array}
        \end{pmatrix}.
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item For a diagonalizable $n \times n$ matrix $A$, show that $\det \left(e^A\right) = \det e^{{\rm trace}\,\, A}$, where $e^A$ is the matrix exponential of $A$: $e^A = \sum^\infty_{k = 0} \frac{1}{k!} A^k$.
    
    \item Now for an arbitrary $2 \times 2$ matrix $A$ with trace equal to $0$, show that $\det \left(e^A\right) = 1$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A$ is diagonalizable, then there exists an invertible matrix $S$ such that $A = S \Lambda S^{-1}$. Then, $e^A = S e^\Lambda S^{-1}$, and we have
    \begin{align*}
        \det e^A = \det e^\Lambda = \prod^J_{j=1} e^{\lambda_j} = \det e^{{\rm trace}\,\, A},
    \end{align*}
    where $\lambda_j$ is eigenvalue of $A$ and in the last step we usd $\sum^J_{i = 1} \lambda_j = {\rm trace}\,\, A$\cite{5}. \footnote{Suppose $A = S J_A S^{-1}$, where $J_A$ is Jordan form of $A$ with diagonal entries $\lambda_j$. Then, ${\rm tr} (A) = {\rm tr} \left(S J_A S^{-1} \right) = {\rm tr} \left(S S^{-1} J_A \right) = {\rm tr} \left(J_A \right) = \sum \lambda_j$.}
    
    \item \begin{enumerate}[label = \arabic*)]
        \item For $A$ diagonalizable, then it follows from $(a)$. 
        
        \item For $A$ non-diagonalizable and ${\rm trace}\,\, A = 0$, then we suppose 
        \begin{align*}
            A = \begin{pmatrix}
                - a &  b \\
                c   &  a
            \end{pmatrix}. 
        \end{align*}
        Since $A$ is non-diagonalizable, then $A$ has generalized eigenvector, which implies $\lambda^2 = a^2 + bc = 0$. Then, the Jordan Canonical form of $A$ has following two possibilities
        \begin{align*}
            J_A = \begin{pmatrix}
                0 &  1 \\
                0   &  0
            \end{pmatrix}\,\, {\rm or} \,\, \begin{pmatrix}
                0 &  0 \\
                0   &  0
            \end{pmatrix}.
        \end{align*}
        Thus, we have 
        \begin{align*}
            \det \left(e^A\right) = \det \left( \sum^\infty_{k = 0} \frac{1}{k!} A^k \right) = \det \left(A^0\right) = 1.
        \end{align*}
    \end{enumerate}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item Show that if the self-adjoint part of a matrix $A$ is positive definite, then $A$ is invertible and the self-adjoint part of $A^{-1}$ is positive definite.
    
    \item Let $a$ be a fixed positive real number. Show that if a self-adjoint matrix $A$ is positive definite, then $\|W\| \leq 1$, where $W = (I - a A)(I + a A)^{-1}$ and $\|\cdot\|$ is the operator norm induced by Euclidean norm.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A$ is self-adjoint, then $A$ can be unitary diagonalized, i.e., there exists orthogonal matrix $Q$ such that $A = Q \Lambda Q^T$, where \begin{align*}
        \Lambda = \begin{pmatrix}
            \lambda_1 &  & \\
            & \ddots &     \\
            & & \lambda_n
        \end{pmatrix}.
    \end{align*}
    Since $A$ is positive definite, Then $\lambda_j > 0, 1 \leq j \leq n$. Thus, $\det A > 0$, and hence $A$ is invertible.
    
    Also, eigenvalues of $A^{-1}$ are $1/\lambda_j$, and thus $A^{-1}$ is also positive definite.
    
    \item $W = (I - a A)(I + a A)^{-1}$, then 
    \begin{align*}
        W & = Q(I - a \Lambda)Q^T \left(Q(I + a \Lambda)Q^T\right)^{-1} \\
        & = Q (I - a \Lambda)(I + a \Lambda)^{-1} Q^T \\
        & = Q \begin{pmatrix}
            \frac{1 - a\lambda_1}{1 + a\lambda_1} &  & \\
            & \ddots &     \\
            & & \frac{1 - a\lambda_n}{1 + a\lambda_n}
        \end{pmatrix} Q^T := Q \overline{\Lambda} Q^T.
    \end{align*}
    Thus, for any $x \in \mathbb{R}^n$, and then 
    \begin{align*}
        (x, W x) = \left(x, Q \overline{\Lambda} Q^T x\right) = \left(Q^Tx, \overline{\Lambda} Q^T x\right) \leq \|x\|,
    \end{align*}
    since $\left|\frac{1 - a\lambda_j}{1 + a\lambda_j}\right| < 1$, and hence $\|W\| = \sup \frac{(x, W x)}{\|x\|} < 1$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Consider
\begin{align*}
    B = \begin{pmatrix}
        L & M \\
        O & N
    \end{pmatrix} \in \mathbb{C}^{2n \times 2n},
\end{align*}
for $L,M,N,O \in \mathbb{C}^{n \times n}$ such that $O$ is the zero matrix.
\begin{enumerate}[label=(\alph*)]
    \item Show that if $B$ diagonalizable, then $L$ and $M$ must be diagonalizable.
    
    \item Show that if $L$ and $M$ are diagonalizable and does not share eigenvalues, then $B$ is also diagonalizable.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item $A$ is diagonalizable if and only if $m_A(x) = 0$ has distinct roots. We want to show $m_L(x) = 0$ and $m_N(x) = 0$ have distinct roots.
    
    Also, for any polynomial $P(x)$, we have
    \begin{align*}
        P(A) = \begin{pmatrix}
            P(L) & * \\
            O    & P(N)
        \end{pmatrix},
    \end{align*}
    where the upper block $*$ denotes some polynomial of $L, M$ and $N$. Then, 
    \begin{align*}
        m_A(A) = \begin{pmatrix}
            m_L(L) & * \\
            O    & m_N(N)
        \end{pmatrix} = 0,
    \end{align*}
    which implies $m_L(L) = 0$ and $m_N(N) = 0$\cite{6}. Also, we have $m_L(x)|m_A(x)$ and $m_N(x)|m_A(x)$. Then, $m_L(L) = 0$ and $m_N(N) = 0$ have distinct roots. Thus, $L$ and $M$ are diagonalizable.
    
    \item By assumption, $m_L(x) = 0$ and $m_N(x) = 0$ have distinct roots. And $m(x) = m_L(x) m_N(x)$ has distinct roots. 
    
    We want to show $m_A(A) = 0$ since then, with $m_A(x)|m(x)$, $m_A(A)$ has distinct roots. Indeed, we have
    \begin{align*}
        m_A(A) = m_L(A)m_N(A) = \begin{pmatrix}
            m_L(L) & * \\
            O      & m_L(N)
        \end{pmatrix}
        \begin{pmatrix}
            m_N(L) & * \\
            O      & m_N(N)
        \end{pmatrix} = 0.
    \end{align*}
    Thus, $A$ is diagonalizable.
\end{enumerate}
\end{proof}

\newpage

\section{August 2018 Exam}

\begin{exercise}\label{aug_2018_1}
Let $A$ be a Hermitian matrix and $B = \Re A$, the real part of $A$. Show that
\begin{align*}
    \max_{\mu \in \sigma(B)} \mu \leq \max_{\lambda \in \sigma(A)} \lambda,
\end{align*}
where $\sigma(A)$ and $\sigma(B)$ are the spectrums of $A$ and $B$ respectively.
\end{exercise}
\begin{proof}
Let $B = \frac{A + \bar{A}}{2}$, and both $B$ and $\bar{A}$ are both Hermitian matrices. Also, largest eigenvalues of $A$ and $B$ can be written as
\begin{align*}
    \max_{\lambda \in \sigma(A)} \lambda = \max_{x \neq 0} \frac{(x, Ax)}{(x,x)}, \quad \max_{\mu \in \sigma(B)} \mu = \max_{x \neq 0} \frac{(x, Bx)}{(x,x)}.
\end{align*}
Then, with $B = \frac{A + \bar{A}}{2}$, we have
\begin{align*}
    \max_{x \neq 0} \frac{(x, Bx)}{(x,x)} & = \max_{x \neq 0} \left[\frac{1}{2} \frac{(x, Ax)}{(x,x)} + \frac{1}{2} \frac{(x, \bar{A}x)}{(x,x)} \right] \\
    & \leq \frac{1}{2} \max_{x \neq 0} \frac{(x, Ax)}{(x,x)} + \frac{1}{2} \max_{x \neq 0} \frac{(x, \bar{A}x)}{(x,x)} \\
    & \leq \max_{x \neq 0} \frac{(x, Ax)}{(x,x)},
\end{align*}
where in the last step we used the fact that $\max \frac{(x, Ax)}{(x,x)} = \max \frac{(x, \Bar{x}x)}{(x,x)}$, since $\Bar{A}$ is conjugate transpose of $A$ and it has the same eigenvalues as $A$. Thus, $\max \mu \leq \max \lambda$.
\end{proof}

\begin{proof}[Second Proof of Exercise \ref{aug_2018_1}]
Let $\mu_1 = \max \mu$, and $\mu_1 y = By$. And let $C = A - B$, then $C$ is purely imaginary Hermitiain matrix. Then,
\begin{align*}
    \max_{x \neq 0} \frac{(x, Ax)}{(x,x)} \geq \frac{(y, Ay)}{(y,y)} = \frac{(y, By) + (y, Cy)}{(y,y)} = \mu_1 + 0.
\end{align*}
Thus, $\mu_1 = \max \mu \leq \max \lambda$.
\end{proof}

\begin{remark}
There is a similar result, $\min_{\lambda \in \sigma(A)} \lambda \leq \min_{\mu \in \sigma(B)} \mu$.
\end{remark}

\medskip

\begin{exercise}{\rm *}
Suppose $T_1, \cdots, T_{n+1}$ are pairwise commuting linear operators on vector space $V$ of dimension $n$. Suppose 
\begin{align*}
    T_1 T_2 \cdots T_{n+1} = 0.
\end{align*}
Prove that in the above equation at least one of the factors can be removed without changing its validity.
\end{exercise}
\begin{proof}
Consider the following relations between subspaces\cite{7}:
\begin{align*}
    0 = I_0 \subset I_1 \subset \cdots \subset I_n = V,
\end{align*}
where $I_i$ is image of $T_{i+1}\cdots T_{n+1}$ for $i = 0, 1, \cdots, n$. Indeed, since $T_{i+1}$ commutes with $T_{i+2}, \cdots$, $T_{n+1}$, then 
\begin{align*}
    I_i = \Im (T_{i+1}\cdots T_{n+1}) = \Im (T_{i+2}\cdots T_{n+1} T_{i+1}) \subset \Im (T_{i+2}\cdots T_{n+1}) = I_{i+1},
\end{align*}
since $\Im(AB) \subset \Im(A)$. Note that $I_i = T_{i+1}I_{i+1}$.

Let $d_i = \dim (I_i)$, then we get a weakly increasing sequence of integers
\begin{align*}
    0 = d_0 \leq d_1 \leq \cdots \leq d_n \leq n.
\end{align*}
If $d_n = \dim T_{n+1} = n$, then we are done, since $T_{n+1}$ is full rank and can be removed in the above equation. If not, then $d_n = \dim T_{n+1} \leq n - 1$, then there must be a integer $k \leq n$ such that $d_k = d_{k+1}$. Then, $I_{k} = I_{k+1}$, and $T_k$ can be removed. Indeed,
\begin{align*}
    0 & = T_1 T_2 \cdots T_k T_{k+1} (I_{k+1}) \\
    & = T_1 T_2 \cdots T_k (I_k) \\
    & = T_1 T_2 \cdots T_k (I_{k+1}) \\
    & = T_1 T_2 \cdots T_k T_{k+2} \cdots T_{n+1}.
\end{align*}
\end{proof}

\medskip

\begin{exercise}
Let $A, B$ be two square complex matrices.
\begin{enumerate}[label=(\alph*)]
    \item Prove that if $A$ and $B$ are diagonalizable and $AB = BA$, then
    \begin{align*}
        e^{A + B} = e^A e^B.
    \end{align*}
    
    \item Prove that the conclusion of part (a) is false if $A$ and $B$ are both diagonalizable but do not commute.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $AB = BA$, then
    \begin{align*}
        e^{A+B} & = \sum^\infty_{k=0} \frac{(A+B)^k}{k!} \\
        & = \sum^\infty_{k=0} \sum^k_{j=0} \frac{\binom{k}{j}A^k B^{k-j}}{k!} \\
        & = \sum^\infty_{k=0} \sum^k_{j=0} \frac{A^k B^{k-j}}{j!(k-j)!} \\
        & = \sum^\infty_{k=0} \frac{A^k}{k!} \sum^\infty_{j=0} \frac{B^j}{j!} = e^A e^B.
    \end{align*}
    
    \item If $AB \neq BA$, then $(A + B)^k \neq \sum^k_{j=0}\binom{k}{j}A^k B^{k-j}$. Thus the equality does not hold in general.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $A$ and $B$ be two $n \times n$ real orthogonal matrices. Prove that if
\begin{align*}
    \det A + \det B = 0,
\end{align*}
then 
\begin{align*}
    \det(A + B) = 0.
\end{align*}
\end{exercise}
\begin{proof}
Since $A$ and $B$ be two $n \times n$ real orthogonal matrices, then $\det A = \pm 1$ and also $\det B = \pm 1$. Since $\det A + \det B = 0$, then $\det A \det B = \det (AB) = -1$. Now, we have
\begin{align*}
    \det (A + B) & = \det A\left(A^T + B^T\right)B \\
    & = \det A \det B \det \left(A^T + B^T\right) \\
    & = - \det (A + B)^T \\
    & = - \det (A + B),
\end{align*}
and hence $\det (A + B) = 0$.
\end{proof}

\medskip

\begin{exercise}
Prove that a complex $3 \times 3$ matrix $A$ is nilpotent if and only if $\Tr \left(A^k\right) = 0$ for $k = 1, 2, 3$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item ($\Rightarrow$) Since $A$ is nilpotent, then all eigenvalues are zero. Then, $\Tr \left(A\right) = 0$. Also, $A$ is unitary similar to an upper triangular matrix $T$, whose diagonal entries are eigenvalues, and it follows naturally that $\Tr \left(A^k\right) = 0, k = 2, 3$.
    
    \item Denote by $x, y, z$ the eigenvalues of $A$, and consider upper triangular matrix $T$ which is similar to $A$. Since $\Tr \left(A^k\right) = 0$ for $k = 1, 2, 3$, then so does $T$. Then,
    \begin{align*}
        \begin{cases}
            x + y + z = 0, \\
            x^2 + y^2 + z^2 = 0, \\
            x^3 + y^3 + z^3 = 0.
        \end{cases}
    \end{align*}
    By solving this, we have that the solution is $x = y = z = 0$. Thus, $A$ is nilpotent.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
For an invertible matrix $P \in \mathbb{R}^{n \times n}$, let $T_P: \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}$ be the linear map defined by $T_P(M):= PMP^{-1}$ for any $M \in \mathbb{R}^{n \times n}$.
\begin{enumerate}[label=(\alph*)]
    \item For an orthogonal matrix $O$, show that the space $S(n) \subset \mathbb{R}^{n \times n}$ of symmetric $n \times n$ matrices is invariant under $T_O$.
    
    \item Now let $O$ be the $3 \times 3$ rotation matrix:
    \begin{align*}
        O = \begin{pmatrix}
            0 & -1 & 0 \\
            1 & 0  & 0 \\
            0 & 0  & 1
        \end{pmatrix}.
    \end{align*}
    Compute the minimal and characteristic polynomials of $T_O$ on $S(3)$.
\end{enumerate}
{\bf Hint:} Note that $O^4 = I$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $O$ is orthogonal matrix, then for any $S \in S(n)$, $T_O = OSO^{-1} = OSO^T$. Then, $\left(OSO^T\right)^T = OS^TO^T = OSO^T$, since $S$ is symmetric. Thus, $T_O$ is also a symmetric matrix, and hence $S(n)$ is invariant under $T_O$.
    
    \item* Note that 
    \begin{align*}
        T_{O_1}T_{O_2}(S) = O_1 O_2 S O_2^T O_1^T = T_{O_1 O_2}(S),
    \end{align*}
    then by $O^4 = I$, we have $T_{O^4}(S) = T_{O}^4(S) = S$. Then we have $T_{O}^4 = I$, which gives eigenvalues of $T_O$, $\lambda = \{1, -1, i, -i\}$.
    
    Let $S = \begin{pmatrix}
        a & b & c \\
        b & d & e \\
        c & e & f
    \end{pmatrix}$, and for $\lambda = 1$, we have
    \begin{align*}
        T_O(S) = \begin{pmatrix}
        d & -b & -e \\
        -b & a & c \\
        -e & c & f
    \end{pmatrix} = \begin{pmatrix}
        a & b & c \\
        b & d & e \\
        c & e & f
    \end{pmatrix} = S,
    \end{align*}
    which gives the eigenspace
    \begin{align*}
        \left\{\begin{pmatrix}
        1 & &  \\
        & 1 &  \\
        & & 0
    \end{pmatrix}, \begin{pmatrix}
        0 & &  \\
        & 0 &  \\
        & & 1
    \end{pmatrix}\right\}.
    \end{align*}
    For $\lambda = -1$, we have
    \begin{align*}
        T_O(S) = \begin{pmatrix}
        d & -b & -e \\
        -b & a & c \\
        -e & c & f
    \end{pmatrix} = \begin{pmatrix}
        -a & -b & -c \\
        -b & -d & -e \\
        -c & -e & -f
    \end{pmatrix} = -S,
    \end{align*}
    which gives the eigenspace
    \begin{align*}
        \left\{\begin{pmatrix}
        1 & &  \\
        & -1 &  \\
        & & 0
    \end{pmatrix}, \begin{pmatrix}
        1 & 1  & 0 \\
        1 & -1 & 0 \\
        0 & 0  & 0
    \end{pmatrix}\right\}.
    \end{align*}
    Also, since $\dim S(3) = 6$, then the characteristic polynomials of $T_O$ on $S(3)$ is 
    \begin{align*}
        p_\lambda = (\lambda - 1)^2 (\lambda + 1)^2 \left(\lambda^2 + 1\right).
    \end{align*}
    And the minimal polynomial of $T_O$ on $S(3)$ is
    \begin{align*}
        m(\lambda) = (\lambda - 1) (\lambda + 1) (\lambda^2 + 1).
    \end{align*}
\end{enumerate}
\end{proof}


\newpage

\section{May 2018 Exam}

\begin{exercise}
Let $A \in \mathbb{R}^{n \times n}$ be a matrix whose components are either $1$ or $-1$. Prove that $\det A = 2^{n-1} m$, where $m$ is an integer.
\end{exercise}
\begin{proof}
We prove it by induction.
\begin{enumerate}[label=(\alph*)]
    \item When $n = 1$, then the claim follows naturally, since $\det A = \pm 1 = 2^{0} (\pm 1)$.
    
    \item Suppose the claim is true for $n = k$, then for any $A_k \in \mathbb{R}^{k \times k}$, $\det A_k = 2^{k-1}m$ for some integer $m$. Now we consider $A_{k+1} \in \mathbb{R}^{(k+1) \times (k+1)}$, with entries $a_{ij}$, and without losing any generality, we could assume that $a_{11} = 1$.
    
    If $a_{12} = 1$, then we add $-1$ times of the second column to the first one, if $a_{12} = -1$, then we add the second column to the first one and we get a new matrix $\widetilde{A}_{k+1}$. Then, for $\widetilde{A}_{k+1}$, $a_{11} = 0$ and $a_{i1} = 0, 2$ or $-2$ for all $2 \leq i \leq k+1$.
    
    By Laplace expansion, we expand $\widetilde{A}_{k+1}$ through the first column, then we have
    \begin{align*}
        \det A_{k+1} = \det \widetilde{A}_{k+1} & = \sum^{k+1}_{i=1} (-1)^{i+1} a_{i1} \widetilde{A}'_{ik} \\
        & = 2 \sum^l_{a_{i1}=2} (-1)^{i+1} \widetilde{A}'_{ik} + 2 \sum^p_{a_{i1}=-2} (-1)^{i+2} \widetilde{A}'_{ik} \\
        & = 2 \sum^l_{a_{i1}=2} (-1)^{i+1} 2^{k-1} m_i + 2 \sum^p_{a_{i1}=-2} (-1)^{i+2} 2^{k-1} m_i \\
        & = 2^k \left(\sum^l_{a_{i1}=2} (-1)^{i+1} m_i + \sum^p_{a_{i1}=-2} (-1)^{i+2} m_i \right) = 2^k m_{k+1},
    \end{align*}
    where $\widetilde{A}'_{ik}$ is the minor of the entry in $i$th row and the first column of matrix $\widetilde{A}$, and by hypothesis, $\det \widetilde{A}'_{ik} = 2^{k-1} m_i$ for some integer $m_i$. Also, the sum of integers is still integer. Thus, by induction, the claim follows.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Assume that $V$ is a finite dimensional complex vector space. Suppose $T, U \in \mathscr{L}(V, V)$ are two operators and that $TU - UT$ is nonnegative. Prove that $T$ and $U$ have a common eigenvector.
\end{exercise}
\begin{proof}
Let $\dim V = n$. Since $TU - UT$ is nonnegative, then there exists an orthogonal basis $\mathcal{B}$ under which $TU - UT$ is a diagonal matrix with nonnegative diagonal entries. 

Also, since
\begin{align*}
    \Tr (TU - UT) = \Tr(TU) - \Tr(UT) = 0,
\end{align*}
then $TU - UT$ is zero matrix under basis $\mathcal{B}$. 

Now, since $TU = UT$, then we claim there exists a basis of $V$ which consists of eigenvectors and generalized eigenvectors of both $T$ and $U$. Indeed, let $\{\lambda_j \}^K_{j=1}$ be $K$ distinct eigenvalues of $U$, then $V = \oplus^K_{j=1}N_j$, where $N_j$ is null space of $(U - \lambda_j I)^{d_j}$ and $d_j = d(\lambda_j)$ is the index of $\lambda_j$. For any $x \in V$, since $TU = UT$, then 
\begin{align*}
    T (U - \lambda_j I)^{d_j} x = (U - \lambda_j I)^{d_j} T x.
\end{align*}
If $x \in N_j$, then $(U - \lambda_j I)^{d_j} x = 0$, which implies $Tx \in N_j$. Then, $T$ is a map from $N_j$ into $N_j$. By Spectral theorem, $T|_{N_j}$ forms a spectral decomposition of each $N_j$, thus, $N_j$ has a basis consisting of eigenvectors and  generalized eigenvectors of $T$.
\end{proof}

\medskip

\begin{exercise}\label{May_2018_3}
Prove that for any two matrices, $A, B \in \mathbb{R}^{n \times n}$,
\begin{align*}
    \det (I - AB) = \det (I - BA).
\end{align*}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $A$ is invertible, then 
    \begin{align*}
        \det (I - AB) = \det \left(A^{-1} (I - AB ) A\right) = \det (I - BA).
    \end{align*}
    
    \item If $A$ is not invertible, then there exists $\varepsilon_k \to 0$ as $k \to \infty$ such that $\det (A + \varepsilon_k I) \neq 0$. Then, we have
    \begin{align*}
        \det (I -  AB) & = \lim_{k\to\infty} \det \left(I - (A + \varepsilon_k I) B\right) \\
        & = \lim_{k\to\infty} \det \left(I - B(A + \varepsilon_k I)\right) \\
        & = \det (I - BA).
    \end{align*}
\end{enumerate}
\end{proof}

\begin{proof}[Second Proof of Exercise \ref{May_2018_3}]
Since 
\begin{align*}
    \begin{pmatrix} I & A \\ B & I \end{pmatrix} & = \begin{pmatrix} I & O \\ B & I \end{pmatrix} \begin{pmatrix} I & O \\ O & I - BA \end{pmatrix} \begin{pmatrix} I & A \\ O & I \end{pmatrix}, \\
    \begin{pmatrix} I & A \\ B & I \end{pmatrix} & = \begin{pmatrix} I & A \\ O & I \end{pmatrix} \begin{pmatrix} I - AB & O \\ O & I \end{pmatrix} \begin{pmatrix} I & O \\ B & I \end{pmatrix},
\end{align*}
and 
\begin{align*}
    \det \begin{pmatrix} I & O \\ B & I \end{pmatrix} = \det \begin{pmatrix} I & A \\ O & I \end{pmatrix} = 1,
\end{align*}
then we have
\begin{align*}
    \det \begin{pmatrix} I & O \\ O & I - BA \end{pmatrix} = \det \begin{pmatrix} I - AB & O \\ O & I \end{pmatrix},
\end{align*}
which is equivalent to
\begin{align*}
    \det (I - BA) = \det (I - AB).
\end{align*}
\end{proof}

\medskip

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item Assume $A \in \mathbb{C}^{n \times n}$ has $n$ distinct nonzero eigenvalues. Prove that there are exactly $2n$ distinct matrices $B$ such that $B^2 = A$(i.e., in particular, there are no more than $2n$ matrices with this property).
    
    \item How many such matrices $B \in \mathbb{C}^{3 \times 3}$ exist if $A = \operatorname{diag}(2, 2, 1)$?
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A$ has $n$ distinct eigenvalues, then $A$ can be unitarily diagonalizable, such that there exists unitary matrix $U$ such that $A = U \Lambda U^*$, where $\Lambda = \operatorname{diag}(\lambda_1, \cdots, \lambda_n)$ and $\lambda_i \neq 0, i = 1, \cdots, n$. Then, $U^* A U = \Lambda$.
    
    Now let $S = U^* B U$, and then $U^* B^2 U = S^2 = \Lambda$. And since $\Lambda$ is diagonal, then $S$ commutes with $\Lambda$. Suppose
    \begin{align*}
        S = \begin{pmatrix}
            s_{11} & \cdots & s_{1n} \\
            \vdots & \ddots & \vdots \\
            s_{n1} & \cdots & s_{nn}
        \end{pmatrix},
    \end{align*}
    and then $S \Lambda = \Lambda S$ implies $s_{ij} = 0$ for $i \neq j$. Indeed, $s_{ij} \lambda_j = \lambda_i s_{ij}$ and $\lambda_i \neq \lambda_j$. Therefore, $S = \operatorname{diag}(s_{11}, \cdots, s_{nn})$.
    
    Since $S^2 = \Lambda$, then $s_{ii} = \pm \sqrt{\lambda_i}$. Then, there are exactly $2^n$ choices for $S$, and thus there are exactly $2^n$ matrices $B$ such that $B^2 = A$.
    
    \item Let 
    \begin{align*}
        B = \begin{pmatrix}
            b_{11} & b_{12} & b_{13} \\
            b_{21} & b_{22} & b_{23} \\
            b_{31} & b_{31} & b_{33}
        \end{pmatrix},
    \end{align*}
    and since $B$ and $A$ commutes, then we have 
    \begin{align*}
        \begin{pmatrix}
            2b_{11} & 2b_{12} & b_{13} \\
            2b_{21} & 2b_{22} & b_{23} \\
            2b_{31} & 2b_{31} & b_{33}
        \end{pmatrix} = \begin{pmatrix}
            2b_{11} & 2b_{12} & 2b_{13} \\
            2b_{21} & 2b_{22} & 2b_{23} \\
            b_{31} & b_{31} & b_{33}
        \end{pmatrix},
    \end{align*}
    which gives 
    \begin{align*}
        B = \begin{pmatrix}
            b_{11} & b_{12} & 0 \\
            b_{21} & b_{22} & 0 \\
            0 & 0 & b_{33}
        \end{pmatrix}.
    \end{align*}
    
    Since $B^2 = A$, then we have equations
    \begin{align*}
        \begin{cases}
            b_{11}^2 + b_{12}b_{21} = 0, \\
            b_{11} b_{12} + b_{12} b_{22} = 0, \\
            b_{11} b_{21} + b_{21} b_{22} = 0, \\
            b_{22}^2 + b_{12}b_{21} = 0,
        \end{cases}
    \end{align*}
    and if $b_{22} = 0, b_{21} \neq 0$, then the equations reduces to $b_{11} = - b_{22} = 0$ and $b_{12}b_{21} = 0$, which gives infinitely choices for $b_{21}$ and $b_{12}$. Thus, in this case, there exist infinitely many matrices $B$ such that $B^2 = A = \operatorname{diag}(2, 2, 1)$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $f: \mathbb{C}^{n+1} \times \mathbb{C}^{n+1} \to \mathbb{C}$ be the function defined for all 
\end{exercise}













\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}