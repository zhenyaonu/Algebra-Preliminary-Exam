\documentclass[11pt]{article}
\pagestyle{plain}
%\documentclass{article}
%\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}

\usepackage{latexsym,amsmath,amssymb}
\usepackage{amsthm}
%\usepackage[notref,notcite]{showkeys}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{thmtools}
\usepackage{wrapfig}
\usepackage{extarrows}
\usepackage{breqn}
\usepackage{physics}
\usepackage{afterpage}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{mathrsfs}
\usepackage{scalerel}
\usepackage{stackengine,wasysym}
\usepackage{aligned-overset}
\usepackage{stackengine}
\usepackage{mathtools}
\usepackage{nccmath}
\usepackage{url}
\graphicspath{ {images/} }

\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = blue,
    filecolor = magenta,      
    urlcolor = blue,
    citecolor = blue,
}

\urlstyle{same}


\setlength{\oddsidemargin}{1pt}
\setlength{\evensidemargin}{1pt}
\setlength{\marginparwidth}{30pt} % these gain 53pt width
\setlength{\topmargin}{1pt}       % gains 26pt height
\setlength{\headheight}{1pt}      % gains 11pt height
\setlength{\headsep}{1pt}         % gains 24pt height
%\setlength{\footheight}{12 pt} 	  % cannot be changed as number must fit
\setlength{\footskip}{24pt}       % gains 6pt height
\setlength{\textheight}{650pt}    % 528 + 26 + 11 + 24 + 6 + 55 for luck
\setlength{\textwidth}{460pt}     % 360 + 53 + 47 for luck

\title{Sections and Chapters}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{remark}{Remark}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\numberwithin{equation}{subsection}

\def\dsp{\def\baselinestretch{1.35}\large
\normalsize}
%%%%This makes a double spacing. Use this with 11pt style. If you
%%%%want to use this just insert \dsp after the \begin{document}
%%%%The correct baselinestretch for double spacing is 1.37. However
%%%%you can use different parameter.


\def\U{{\mathcal U}}

\begin{document}

\centerline{\Large \bf Algebra Qualifying Exam}
\centerline{Zhen Yao}

\bigskip

\section{May 2019 Exam}

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item Prove or give a counterexample: if $T: \mathbb{R}^3 \to \mathbb{R}^3$ is a linear transformation such that $N_T \cap R_T$ has dimension at least $1$, then $T$ is nilpotent. 
    
    \item Prove or give a counterexample: if $T: \mathbb{R}^4 \to \mathbb{R}^4$ is a linear transformation such that $N_T \cap R_T$ has dimension at least $2$, then $T$ is nilpotent. 
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Consider $T(x,y,z) = (0,x,z)$\cite{1}. Then it is obvious that $T$ is linear. Also, $N_T = \{(0,y,0)| y \in \mathbb{R}\}$ and $R_T = \{(0,x,y)| x,y \in \mathbb{R}\}$. Then, $N_T \cap R_T = \{(0,y,0)| y \in \mathbb{R}\}$, which is at least dimension $1$. However, $T$ is not nilpotent, since 
    \begin{align*}
        T^3(x,y,z) = T^2(0,x,z) = T(0,0,z) = (0,0,z).
    \end{align*}
    
    \item With the rank-nullity theorem, we have 
    \begin{align*}
        \dim N_T + \dim R_T = \dim \mathbb{R}^4 = 4,
    \end{align*}
    also, 
    \begin{align*}
        \dim (N_T + R_T) = \dim N_T + \dim R_T - \dim (N_T \cap R_T) \leq 2.
    \end{align*}
    Then the only possibility is that $\dim N_T = \dim R_T = 2$ and $\dim (N_T \cap R_T) = 2$, which implies $R_T = N_T$\cite{2}. Thus, $T^2(x) = T(T(x)) = 0$, since $T(x) \in R_T$. Hence, $T$ is nilpotent.
\end{enumerate}
\end{proof}

We present another approach to prove part $(b)$.
\begin{proof}
Let $u,v \in N_T \cap R_T$ such that $u, v$ are linearly independent. Then, $Tu = 0$ and $Tv = 0$, and for some $u', v' \in \mathbb{R}^4$, $u = Tu'$ and $v = Tv'$. Define $S = \{u, v, u', v'\}$ and each element of $S$ is annihilated by some power of $T$, saying $T^2$. 

We claim $u, v, u', v'$ are linearly independent. Suppose $c_1 u + c_2 v + c_3 u' + c_4 v' = 0$. We need to prove that $c_i = 0, i = 1,2,3,4$. Applying $T$ to the above equation gives
\begin{align*}
    c_1 T u + c_2 T v + c_3 T u' + c_4 T v' = 0,
\end{align*}
which implies
\begin{align*}
    c_3 u + c_4 v = 0
\end{align*}
and then $c_3 = c_4 = 0$. Thus, $c_1 = c_2 = 0$. Hence, $\{u, v, u', v'\}$ is also nilpotent.
\end{proof}

\medskip

\begin{exercise}
Suppose that $\lambda$ is an eigenvalue of a matrix $A \in \mathbb{C}^{n \times n}$ with algebraic multiplicity $k$. Show that $(A - \lambda I)^k$ has rank $n - k$.
\end{exercise}
\begin{proof}
$A$ is similar to its Jordan Canonial form $J_A(\lambda)$, such that there exists an invertible matrix $S$ such that $A = S J_A S^{-1}$. Suppose under such $S$, $J_A$ has form
\begin{align*}
    J_A = \begin{pmatrix}
        J(\lambda) & 0 & \cdots & 0 \\
        0 & J(\lambda_1) & \cdots & 0 \\
        \vdots & \ddots & \ddots & \vdots \\
        0 & \cdots & \cdots & J(\lambda_l)
    \end{pmatrix},
\end{align*}
where $\lambda, \lambda_1, \cdots, \lambda_l$ are $l+1$ eigenvalues of $A$ and
\begin{align*}
    J(\lambda) = \begin{pmatrix}
        \lambda & 1 & \cdots & \cdots & 0 \\
        0 & \lambda & \cdots & \cdots &  0 \\
        \vdots & \ddots & \ddots & \vdots & 1 \\
        0 & \cdots & \cdots & \cdots & \lambda
    \end{pmatrix}_{k \times k}.
\end{align*}
Also, $(A - \lambda I)^k = S (J_A - \lambda I)^k S^{-1}$, and 
\begin{align*}
    (J(\lambda) - \lambda I) = \begin{pmatrix}
        0 & 1 & \cdots & \cdots & 0 \\
        0 & 0 & \cdots & \cdots &  0 \\
        \vdots & \ddots & \ddots & \vdots & 1 \\
        0 & \cdots & \cdots & \cdots & 0
    \end{pmatrix}_{k \times k}, (J(\lambda) - \lambda I)^k = \begin{pmatrix}
        0 & 0 & \cdots & \cdots & 0 \\
        0 & 0 & \cdots & \cdots &  0 \\
        \vdots & \ddots & \ddots & \vdots & 0 \\
        0 & \cdots & \cdots & \cdots & 0
    \end{pmatrix}_{k \times k}.
\end{align*}
Also, $(J(\lambda_i) - \lambda I)^k \neq 0$ for $i = 1, \cdots, l$, then we have $\dim N_{(J_A - \lambda I)^k} = k$ since there is a $k\times k$ zero matrix in $J_A$. Thus, $\dim R_{(J_A - \lambda I)^k} = n - k$. Since $A$ and $J_A(\lambda)$ are similar, then they has the same eigenvalues and furthermore, $\dim \dim N_{(A - \lambda I)^k} = \dim N_{(J_A - \lambda I)^k}$. The rest follows.
\end{proof}

\medskip

\begin{exercise}{\rm *}
For any $n \geq 1$, classify the matrices $Q \in \mathbb{R}^{n \times n}$ that are both orthogonal and skew-symmetric, meaning $Q^T = -Q$, up to similarity, i.e., exhibit exactly one representative from each real similarity class. \\
{\bf Hint:} The answer is very different for odd versus even $n$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For $n$ is odd, we have $Q^T  = -Q$ and $Q^T Q = I$, Then, $- Q^2 = I$, and then $\det (- Q^2) = 1$. Thus, $(-1)^n \det (Q^2) = 1$. And there is no such matrix.
    
    \item For $n = 2k, k \in \mathbb{N}$ is even, we have the same equaitons as before and $Q^2 + I = 0$. Then the minimal polynomial $m_Q$ of $Q$ satisfies $m_Q(x)|(x^2 + 1)$. 
    
    Also, we claim all eigenvalues of $Q$ are non-zero purely imaginary\cite{3}. Indeed, for $x$ being eigenvector of $Q$ corresponding to eigenvalue $\lambda$, we have 
    \begin{align*}
        \lambda \|x\|^2 = (Ax, x) = (x, A^T x) = (x, - A x) = - \overline{\lambda} \|x\|^2,
    \end{align*}
    thus, $\lambda = -\overline{\lambda}$, which implies $\lambda$ is purely imaginary.
    
    Then, eigenvalues of $Q$ are $i$ and $-i$ since $m_Q(x)|(x^2 + 1)$. Also, $Q$ is diagonalizable\footnote{For every normal matrix $A$, there is a unitary matrix $P$ such that $PAP^{-1}$ is a diagonal matrix. ({\em Linear Algebra}. K. Hoffman  and R.A. Kunze. P317)} in $\mathbb{C}$, since $Q^T Q = Q Q^T = I$, then $Q$ is a normal map and can be unitary diagonalized\cite{4}. Let $x_1, y_1 \in \mathbb{C}^n$ be linear independent, such that for eigenvalue $i$ and $-i$, we have \footnote{Let $Q$ be an orthogonal map on $\mathbb{R}^n$ and suppose $\lambda$ is nonzero eigenvalue of $Q$. Then there exist linearly independent $x,y \in \mathbb{R}^n$ such that for $\lambda = \alpha + i \beta$, we have $Qx = \alpha x - \beta y, Qy = \beta x - \alpha y$.} 
    \begin{align*}
        Q x_1 = y_1, Q y_1 = - x_1.
    \end{align*}
    Let $W_1 = {\rm span}\{x_1, y_1\}$ and $\dim W_1 = 2$. Also, $Q(W_1) \subset W$, and then we have $Q(W_1^\bot) \subset W_1^\bot$ \footnote{Let $Q$ be an orthogonal map on $\mathbb{R}^n$. Let $W \subset \mathbb{R}^n$ such that $Q(W) \subset W$ or $Q(W) = W$, then $Q(W^\bot) \subset W$ or $Q(W^\bot) = W$.}. Thus, $Q|_{W_1^\bot}$ is orthogonal and skew-symmetric since 
    \begin{align*}
        Q|_{W_1^\bot} = \begin{pmatrix}
            \left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right] & 0 \\
            0 & Q_1
        \end{pmatrix}.
    \end{align*}
    Then pick $x_2, y_2 \in \mathbb{C}^n$ be linear independent, such that 
    \begin{align*}
        Q x_2 = y_2, Q y_2 = - x_2,
    \end{align*}
    and $W_2 = {\rm span}\{x_2, y_2\}$. Then, $W_1^\bot = W_2 \oplus W_2^\bot$. Continue this process for $k$ times and we have $\mathbb{R}^n = W_1 \oplus W_2 \cdots \oplus W_k$, where $W_i = {\rm span}\{x_i, y_i\}$ and 
    \begin{align*}
        Q x_i = y_i, Q y_i = - x_i.
    \end{align*}
    Now, let $B = {\rm span}\{x_1, y_1, \cdots, x_k, y_k\}$, then $B$ is a basis of $\mathbb{R}^n$ and 
    \begin{align*}
        [Q]_B = 
        \begin{pmatrix}
            \begin{array}{cccc}
                \left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right] &  \\
                 & \left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right]
            \end{array} & 0 \\
            0 & \begin{array}{cccc}
                \ddots &  \\
                 & \left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right]
            \end{array}
        \end{pmatrix}.
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item For a diagonalizable $n \times n$ matrix $A$, show that $\det \left(e^A\right) = \det e^{{\rm trace}\,\, A}$, where $e^A$ is the matrix exponential of $A$: $e^A = \sum^\infty_{k = 0} \frac{1}{k!} A^k$.
    
    \item Now for an arbitrary $2 \times 2$ matrix $A$ with trace equal to $0$, show that $\det \left(e^A\right) = 1$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A$ is diagonalizable, then there exists an invertible matrix $S$ such that $A = S \Lambda S^{-1}$. Then, $e^A = S e^\Lambda S^{-1}$, and we have
    \begin{align*}
        \det e^A = \det e^\Lambda = \prod^J_{j=1} e^{\lambda_j} = \det e^{{\rm trace}\,\, A},
    \end{align*}
    where $\lambda_j$ is eigenvalue of $A$ and in the last step we usd $\sum^J_{i = 1} \lambda_j = {\rm trace}\,\, A$\cite{5}. \footnote{Suppose $A = S J_A S^{-1}$, where $J_A$ is Jordan form of $A$ with diagonal entries $\lambda_j$. Then, ${\rm tr} (A) = {\rm tr} \left(S J_A S^{-1} \right) = {\rm tr} \left(S S^{-1} J_A \right) = {\rm tr} \left(J_A \right) = \sum \lambda_j$.}
    
    \item \begin{enumerate}[label = \arabic*)]
        \item For $A$ diagonalizable, then it follows from $(a)$. 
        
        \item For $A$ non-diagonalizable and ${\rm trace}\,\, A = 0$, then we suppose 
        \begin{align*}
            A = \begin{pmatrix}
                - a &  b \\
                c   &  a
            \end{pmatrix}. 
        \end{align*}
        Since $A$ is non-diagonalizable, then $A$ has generalized eigenvector, which implies $\lambda^2 = a^2 + bc = 0$. Then, the Jordan Canonical form of $A$ has following two possibilities
        \begin{align*}
            J_A = \begin{pmatrix}
                0 &  1 \\
                0   &  0
            \end{pmatrix}\,\, {\rm or} \,\, \begin{pmatrix}
                0 &  0 \\
                0   &  0
            \end{pmatrix}.
        \end{align*}
        Thus, we have 
        \begin{align*}
            \det \left(e^A\right) = \det \left( \sum^\infty_{k = 0} \frac{1}{k!} A^k \right) = \det \left(A^0\right) = 1.
        \end{align*}
    \end{enumerate}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item Show that if the self-adjoint part of a matrix $A$ is positive definite, then $A$ is invertible and the self-adjoint part of $A^{-1}$ is positive definite.
    
    \item Let $a$ be a fixed positive real number. Show that if a self-adjoint matrix $A$ is positive definite, then $\|W\| \leq 1$, where $W = (I - a A)(I + a A)^{-1}$ and $\|\cdot\|$ is the operator norm induced by Euclidean norm.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A$ is self-adjoint, then $A$ can be unitary diagonalized, i.e., there exists orthogonal matrix $Q$ such that $A = Q \Lambda Q^T$, where \begin{align*}
        \Lambda = \begin{pmatrix}
            \lambda_1 &  & \\
            & \ddots &     \\
            & & \lambda_n
        \end{pmatrix}.
    \end{align*}
    Since $A$ is positive definite, Then $\lambda_j > 0, 1 \leq j \leq n$. Thus, $\det A > 0$, and hence $A$ is invertible.
    
    Also, eigenvalues of $A^{-1}$ are $1/\lambda_j$, and thus $A^{-1}$ is also positive definite.
    
    \item $W = (I - a A)(I + a A)^{-1}$, then 
    \begin{align*}
        W & = Q(I - a \Lambda)Q^T \left(Q(I + a \Lambda)Q^T\right)^{-1} \\
        & = Q (I - a \Lambda)(I + a \Lambda)^{-1} Q^T \\
        & = Q \begin{pmatrix}
            \frac{1 - a\lambda_1}{1 + a\lambda_1} &  & \\
            & \ddots &     \\
            & & \frac{1 - a\lambda_n}{1 + a\lambda_n}
        \end{pmatrix} Q^T := Q \overline{\Lambda} Q^T.
    \end{align*}
    Thus, for any $x \in \mathbb{R}^n$, and then 
    \begin{align*}
        (x, W x) = \left(x, Q \overline{\Lambda} Q^T x\right) = \left(Q^Tx, \overline{\Lambda} Q^T x\right) \leq \|x\|,
    \end{align*}
    since $\left|\frac{1 - a\lambda_j}{1 + a\lambda_j}\right| < 1$, and hence $\|W\| = \sup \frac{(x, W x)}{\|x\|} < 1$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Consider
\begin{align*}
    B = \begin{pmatrix}
        L & M \\
        O & N
    \end{pmatrix} \in \mathbb{C}^{2n \times 2n},
\end{align*}
for $L,M,N,O \in \mathbb{C}^{n \times n}$ such that $O$ is the zero matrix.
\begin{enumerate}[label=(\alph*)]
    \item Show that if $B$ diagonalizable, then $L$ and $M$ must be diagonalizable.
    
    \item Show that if $L$ and $M$ are diagonalizable and does not share eigenvalues, then $B$ is also diagonalizable.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item $A$ is diagonalizable if and only if $m_A(x) = 0$ has distinct roots. We want to show $m_L(x) = 0$ and $m_N(x) = 0$ have distinct roots.
    
    Also, for any polynomial $P(x)$, we have
    \begin{align*}
        P(A) = \begin{pmatrix}
            P(L) & * \\
            O    & P(N)
        \end{pmatrix},
    \end{align*}
    where the upper block $*$ denotes some polynomial of $L, M$ and $N$. Then, 
    \begin{align*}
        m_A(A) = \begin{pmatrix}
            m_L(L) & * \\
            O    & m_N(N)
        \end{pmatrix} = 0,
    \end{align*}
    which implies $m_L(L) = 0$ and $m_N(N) = 0$\cite{6}. Also, we have $m_L(x)|m_A(x)$ and $m_N(x)|m_A(x)$. Then, $m_L(L) = 0$ and $m_N(N) = 0$ have distinct roots. Thus, $L$ and $M$ are diagonalizable.
    
    \item By assumption, $m_L(x) = 0$ and $m_N(x) = 0$ have distinct roots. And $m(x) = m_L(x) m_N(x)$ has distinct roots. 
    
    We want to show $m_A(A) = 0$ since then, with $m_A(x)|m(x)$, $m_A(A)$ has distinct roots. Indeed, we have
    \begin{align*}
        m_A(A) = m_L(A)m_N(A) = \begin{pmatrix}
            m_L(L) & * \\
            O      & m_L(N)
        \end{pmatrix}
        \begin{pmatrix}
            m_N(L) & * \\
            O      & m_N(N)
        \end{pmatrix} = 0.
    \end{align*}
    Thus, $A$ is diagonalizable.
\end{enumerate}
\end{proof}

\newpage

\section{August 2018 Exam}

\begin{exercise}\label{aug_2018_1}
Let $A$ be a Hermitian matrix and $B = \Re A$, the real part of $A$. Show that
\begin{align*}
    \max_{\mu \in \sigma(B)} \mu \leq \max_{\lambda \in \sigma(A)} \lambda,
\end{align*}
where $\sigma(A)$ and $\sigma(B)$ are the spectrums of $A$ and $B$ respectively.
\end{exercise}
\begin{proof}
Let $B = \frac{A + \bar{A}}{2}$, and both $B$ and $\bar{A}$ are both Hermitian matrices. Also, largest eigenvalues of $A$ and $B$ can be written as
\begin{align*}
    \max_{\lambda \in \sigma(A)} \lambda = \max_{x \neq 0} \frac{(x, Ax)}{(x,x)}, \quad \max_{\mu \in \sigma(B)} \mu = \max_{x \neq 0} \frac{(x, Bx)}{(x,x)}.
\end{align*}
Then, with $B = \frac{A + \bar{A}}{2}$, we have
\begin{align*}
    \max_{x \neq 0} \frac{(x, Bx)}{(x,x)} & = \max_{x \neq 0} \left[\frac{1}{2} \frac{(x, Ax)}{(x,x)} + \frac{1}{2} \frac{(x, \bar{A}x)}{(x,x)} \right] \\
    & \leq \frac{1}{2} \max_{x \neq 0} \frac{(x, Ax)}{(x,x)} + \frac{1}{2} \max_{x \neq 0} \frac{(x, \bar{A}x)}{(x,x)} \\
    & \leq \max_{x \neq 0} \frac{(x, Ax)}{(x,x)},
\end{align*}
where in the last step we used the fact that $\max \frac{(x, Ax)}{(x,x)} = \max \frac{(x, \Bar{A}x)}{(x,x)}$, since $\Bar{A}$ is conjugate transpose of $A$ and it has the same eigenvalues as $A$. Thus, $\max \mu \leq \max \lambda$.
\end{proof}

\begin{proof}[Second Proof of Exercise \ref{aug_2018_1}]
Let $\mu_1 = \max \mu$, and $\mu_1 y = By$. And let $C = A - B$, then $C$ is purely imaginary Hermitiain matrix. Then,
\begin{align*}
    \max_{x \neq 0} \frac{(x, Ax)}{(x,x)} \geq \frac{(y, Ay)}{(y,y)} = \frac{(y, By) + (y, Cy)}{(y,y)} = \mu_1 + 0.
\end{align*}
Thus, $\mu_1 = \max \mu \leq \max \lambda$.
\end{proof}

\begin{remark}
There is a similar result, $\min_{\lambda \in \sigma(A)} \lambda \leq \min_{\mu \in \sigma(B)} \mu$.
\end{remark}

\medskip

\begin{exercise}{\rm *}
Suppose $T_1, \cdots, T_{n+1}$ are pairwise commuting linear operators on vector space $V$ of dimension $n$. Suppose 
\begin{align*}
    T_1 T_2 \cdots T_{n+1} = 0.
\end{align*}
Prove that in the above equation at least one of the factors can be removed without changing its validity.
\end{exercise}
\begin{proof}
Consider the following relations between subspaces\cite{7}:
\begin{align*}
    0 = I_0 \subset I_1 \subset \cdots \subset I_n = V,
\end{align*}
where $I_i$ is image of $T_{i+1}\cdots T_{n+1}$ for $i = 0, 1, \cdots, n$. Indeed, since $T_{i+1}$ commutes with $T_{i+2}, \cdots$, $T_{n+1}$, then 
\begin{align*}
    I_i = \Im (T_{i+1}\cdots T_{n+1}) = \Im (T_{i+2}\cdots T_{n+1} T_{i+1}) \subset \Im (T_{i+2}\cdots T_{n+1}) = I_{i+1},
\end{align*}
since $\Im(AB) \subset \Im(A)$. Note that $I_i = T_{i+1}I_{i+1}$.

Let $d_i = \dim (I_i)$, then we get a weakly increasing sequence of integers
\begin{align*}
    0 = d_0 \leq d_1 \leq \cdots \leq d_n \leq n.
\end{align*}
If $d_n = \dim T_{n+1} = n$, then we are done, since $T_{n+1}$ is full rank and can be removed in the above equation. If not, then $d_n = \dim T_{n+1} \leq n - 1$, then there must be a integer $k \leq n$ such that $d_k = d_{k+1}$. Then, $I_{k} = I_{k+1}$, and $T_k$ can be removed. Indeed,
\begin{align*}
    0 & = T_1 T_2 \cdots T_k T_{k+1} (I_{k+1}) \\
    & = T_1 T_2 \cdots T_k (I_k) \\
    & = T_1 T_2 \cdots T_k (I_{k+1}) \\
    & = T_1 T_2 \cdots T_k T_{k+2} \cdots T_{n+1}.
\end{align*}
\end{proof}

\medskip

\begin{exercise}
Let $A, B$ be two square complex matrices.
\begin{enumerate}[label=(\alph*)]
    \item Prove that if $A$ and $B$ are diagonalizable and $AB = BA$, then
    \begin{align*}
        e^{A + B} = e^A e^B.
    \end{align*}
    
    \item Prove that the conclusion of part (a) is false if $A$ and $B$ are both diagonalizable but do not commute.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $AB = BA$, then
    \begin{align*}
        e^{A+B} & = \sum^\infty_{k=0} \frac{(A+B)^k}{k!} \\
        & = \sum^\infty_{k=0} \sum^k_{j=0} \frac{\binom{k}{j}A^k B^{k-j}}{k!} \\
        & = \sum^\infty_{k=0} \sum^k_{j=0} \frac{A^k B^{k-j}}{j!(k-j)!} \\
        & = \sum^\infty_{k=0} \frac{A^k}{k!} \sum^\infty_{j=0} \frac{B^j}{j!} = e^A e^B.
    \end{align*}
    
    \item If $AB \neq BA$, then $(A + B)^k \neq \sum^k_{j=0}\binom{k}{j}A^k B^{k-j}$. Thus the equality does not hold in general.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $A$ and $B$ be two $n \times n$ real orthogonal matrices. Prove that if
\begin{align*}
    \det A + \det B = 0,
\end{align*}
then 
\begin{align*}
    \det(A + B) = 0.
\end{align*}
\end{exercise}
\begin{proof}
Since $A$ and $B$ be two $n \times n$ real orthogonal matrices, then $\det A = \pm 1$ and also $\det B = \pm 1$. Since $\det A + \det B = 0$, then $\det A \det B = \det (AB) = -1$. Now, we have
\begin{align*}
    \det (A + B) & = \det A\left(A^T + B^T\right)B \\
    & = \det A \det B \det \left(A^T + B^T\right) \\
    & = - \det (A + B)^T \\
    & = - \det (A + B),
\end{align*}
and hence $\det (A + B) = 0$.
\end{proof}

\medskip

\begin{exercise}
Prove that a complex $3 \times 3$ matrix $A$ is nilpotent if and only if $\Tr \left(A^k\right) = 0$ for $k = 1, 2, 3$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item ($\Rightarrow$) Since $A$ is nilpotent, then all eigenvalues are zero. Then, $\Tr \left(A\right) = 0$. Also, $A$ is unitary similar to an upper triangular matrix $T$, whose diagonal entries are eigenvalues, and it follows naturally that $\Tr \left(A^k\right) = 0, k = 2, 3$.
    
    \item Denote by $x, y, z$ the eigenvalues of $A$, and consider upper triangular matrix $T$ which is similar to $A$. Since $\Tr \left(A^k\right) = 0$ for $k = 1, 2, 3$, then so does $T$. Then,
    \begin{align*}
        \begin{cases}
            x + y + z = 0, \\
            x^2 + y^2 + z^2 = 0, \\
            x^3 + y^3 + z^3 = 0.
        \end{cases}
    \end{align*}
    By solving this, we have that the solution is $x = y = z = 0$. Thus, $A$ is nilpotent.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
For an invertible matrix $P \in \mathbb{R}^{n \times n}$, let $T_P: \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times n}$ be the linear map defined by $T_P(M):= PMP^{-1}$ for any $M \in \mathbb{R}^{n \times n}$.
\begin{enumerate}[label=(\alph*)]
    \item For an orthogonal matrix $O$, show that the space $S(n) \subset \mathbb{R}^{n \times n}$ of symmetric $n \times n$ matrices is invariant under $T_O$.
    
    \item Now let $O$ be the $3 \times 3$ rotation matrix:
    \begin{align*}
        O = \begin{pmatrix}
            0 & -1 & 0 \\
            1 & 0  & 0 \\
            0 & 0  & 1
        \end{pmatrix}.
    \end{align*}
    Compute the minimal and characteristic polynomials of $T_O$ on $S(3)$.
\end{enumerate}
{\bf Hint:} Note that $O^4 = I$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $O$ is orthogonal matrix, then for any $S \in S(n)$, $T_O = OSO^{-1} = OSO^T$. Then, $\left(OSO^T\right)^T = OS^TO^T = OSO^T$, since $S$ is symmetric. Thus, $T_O$ is also a symmetric matrix, and hence $S(n)$ is invariant under $T_O$.
    
    \item* Note that 
    \begin{align*}
        T_{O_1}T_{O_2}(S) = O_1 O_2 S O_2^T O_1^T = T_{O_1 O_2}(S),
    \end{align*}
    then by $O^4 = I$, we have $T_{O^4}(S) = T_{O}^4(S) = S$. Then we have $T_{O}^4 = I$, which gives eigenvalues of $T_O$, $\lambda = \{1, -1, i, -i\}$.
    
    Let $S = \begin{pmatrix}
        a & b & c \\
        b & d & e \\
        c & e & f
    \end{pmatrix}$, and for $\lambda = 1$, we have
    \begin{align*}
        T_O(S) = \begin{pmatrix}
        d & -b & -e \\
        -b & a & c \\
        -e & c & f
    \end{pmatrix} = \begin{pmatrix}
        a & b & c \\
        b & d & e \\
        c & e & f
    \end{pmatrix} = S,
    \end{align*}
    which gives the eigenspace
    \begin{align*}
        \left\{\begin{pmatrix}
        1 & &  \\
        & 1 &  \\
        & & 0
    \end{pmatrix}, \begin{pmatrix}
        0 & &  \\
        & 0 &  \\
        & & 1
    \end{pmatrix}\right\}.
    \end{align*}
    For $\lambda = -1$, we have
    \begin{align*}
        T_O(S) = \begin{pmatrix}
        d & -b & -e \\
        -b & a & c \\
        -e & c & f
    \end{pmatrix} = \begin{pmatrix}
        -a & -b & -c \\
        -b & -d & -e \\
        -c & -e & -f
    \end{pmatrix} = -S,
    \end{align*}
    which gives the eigenspace
    \begin{align*}
        \left\{\begin{pmatrix}
        1 & &  \\
        & -1 &  \\
        & & 0
    \end{pmatrix}, \begin{pmatrix}
        1 & 1  & 0 \\
        1 & -1 & 0 \\
        0 & 0  & 0
    \end{pmatrix}\right\}.
    \end{align*}
    Also, since $\dim S(3) = 6$, then the characteristic polynomials of $T_O$ on $S(3)$ is 
    \begin{align*}
        p_\lambda = (\lambda - 1)^2 (\lambda + 1)^2 \left(\lambda^2 + 1\right).
    \end{align*}
    And the minimal polynomial of $T_O$ on $S(3)$ is
    \begin{align*}
        m(\lambda) = (\lambda - 1) (\lambda + 1) (\lambda^2 + 1).
    \end{align*}
\end{enumerate}
\end{proof}


\newpage
\section{May 2018 Exam}

\begin{exercise}
Let $A \in \mathbb{R}^{n \times n}$ be a matrix whose components are either $1$ or $-1$. Prove that $\det A = 2^{n-1} m$, where $m$ is an integer.
\end{exercise}
\begin{proof}
We prove it by induction.
\begin{enumerate}[label=(\alph*)]
    \item When $n = 1$, then the claim follows naturally, since $\det A = \pm 1 = 2^{0} (\pm 1)$.
    
    \item Suppose the claim is true for $n = k$, then for any $A_k \in \mathbb{R}^{k \times k}$, $\det A_k = 2^{k-1}m$ for some integer $m$. Now we consider $A_{k+1} \in \mathbb{R}^{(k+1) \times (k+1)}$, with entries $a_{ij}$, and without losing any generality, we could assume that $a_{11} = 1$.
    
    If $a_{12} = 1$, then we add $-1$ times of the second column to the first one, if $a_{12} = -1$, then we add the second column to the first one and we get a new matrix $\widetilde{A}_{k+1}$. Then, for $\widetilde{A}_{k+1}$, $a_{11} = 0$ and $a_{i1} = 0, 2$ or $-2$ for all $2 \leq i \leq k+1$.
    
    By Laplace expansion, we expand $\widetilde{A}_{k+1}$ through the first column, then we have
    \begin{align*}
        \det A_{k+1} = \det \widetilde{A}_{k+1} & = \sum^{k+1}_{i=1} (-1)^{i+1} a_{i1} \widetilde{A}'_{ik} \\
        & = 2 \sum^l_{a_{i1}=2} (-1)^{i+1} \widetilde{A}'_{ik} + 2 \sum^p_{a_{i1}=-2} (-1)^{i+2} \widetilde{A}'_{ik} \\
        & = 2 \sum^l_{a_{i1}=2} (-1)^{i+1} 2^{k-1} m_i + 2 \sum^p_{a_{i1}=-2} (-1)^{i+2} 2^{k-1} m_i \\
        & = 2^k \left(\sum^l_{a_{i1}=2} (-1)^{i+1} m_i + \sum^p_{a_{i1}=-2} (-1)^{i+2} m_i \right) = 2^k m_{k+1},
    \end{align*}
    where $\widetilde{A}'_{ik}$ is the minor of the entry in $i$th row and the first column of matrix $\widetilde{A}$, and by hypothesis, $\det \widetilde{A}'_{ik} = 2^{k-1} m_i$ for some integer $m_i$. Also, the sum of integers is still integer. Thus, by induction, the claim follows.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Assume that $V$ is a finite dimensional complex vector space. Suppose $T, U \in \mathscr{L}(V, V)$ are two operators and that $TU - UT$ is nonnegative. Prove that $T$ and $U$ have a common eigenvector.
\end{exercise}
\begin{proof}
Let $\dim V = n$. Since $TU - UT$ is nonnegative, then there exists an orthogonal basis $\mathcal{B}$ under which $TU - UT$ is a diagonal matrix with nonnegative diagonal entries. 

Also, since
\begin{align*}
    \Tr (TU - UT) = \Tr(TU) - \Tr(UT) = 0,
\end{align*}
then $TU - UT$ is zero matrix under basis $\mathcal{B}$. 

Now, since $TU = UT$, then we claim there exists a basis of $V$ which consists of eigenvectors and generalized eigenvectors of both $T$ and $U$. Indeed, let $\{\lambda_j \}^K_{j=1}$ be $K$ distinct eigenvalues of $U$, then $V = \oplus^K_{j=1}N_j$, where $N_j$ is null space of $(U - \lambda_j I)^{d_j}$ and $d_j = d(\lambda_j)$ is the index of $\lambda_j$. For any $x \in V$, since $TU = UT$, then 
\begin{align*}
    T (U - \lambda_j I)^{d_j} x = (U - \lambda_j I)^{d_j} T x.
\end{align*}
If $x \in N_j$, then $(U - \lambda_j I)^{d_j} x = 0$, which implies $Tx \in N_j$. Then, $T$ is a map from $N_j$ into $N_j$. By Spectral theorem, $T|_{N_j}$ forms a spectral decomposition of each $N_j$, thus, $N_j$ has a basis consisting of eigenvectors and  generalized eigenvectors of $T$.
\end{proof}

\medskip

\begin{exercise}\label{May_2018_3}
Prove that for any two matrices, $A, B \in \mathbb{R}^{n \times n}$,
\begin{align*}
    \det (I - AB) = \det (I - BA).
\end{align*}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $A$ is invertible, then 
    \begin{align*}
        \det (I - AB) = \det \left(A^{-1} (I - AB ) A\right) = \det (I - BA).
    \end{align*}
    
    \item If $A$ is not invertible, then there exists $\varepsilon_k \to 0$ as $k \to \infty$ such that $\det (A + \varepsilon_k I) \neq 0$. Then, we have
    \begin{align*}
        \det (I -  AB) & = \lim_{k\to\infty} \det \left(I - (A + \varepsilon_k I) B\right) \\
        & = \lim_{k\to\infty} \det \left(I - B(A + \varepsilon_k I)\right) \\
        & = \det (I - BA).
    \end{align*}
\end{enumerate}
\end{proof}

\begin{proof}[Second Proof of Exercise \ref{May_2018_3}]
Since 
\begin{align*}
    \begin{pmatrix} I & A \\ B & I \end{pmatrix} & = \begin{pmatrix} I & O \\ B & I \end{pmatrix} \begin{pmatrix} I & O \\ O & I - BA \end{pmatrix} \begin{pmatrix} I & A \\ O & I \end{pmatrix}, \\
    \begin{pmatrix} I & A \\ B & I \end{pmatrix} & = \begin{pmatrix} I & A \\ O & I \end{pmatrix} \begin{pmatrix} I - AB & O \\ O & I \end{pmatrix} \begin{pmatrix} I & O \\ B & I \end{pmatrix},
\end{align*}
and 
\begin{align*}
    \det \begin{pmatrix} I & O \\ B & I \end{pmatrix} = \det \begin{pmatrix} I & A \\ O & I \end{pmatrix} = 1,
\end{align*}
then we have
\begin{align*}
    \det \begin{pmatrix} I & O \\ O & I - BA \end{pmatrix} = \det \begin{pmatrix} I - AB & O \\ O & I \end{pmatrix},
\end{align*}
which is equivalent to
\begin{align*}
    \det (I - BA) = \det (I - AB).
\end{align*}
\end{proof}

\medskip

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item Assume $A \in \mathbb{C}^{n \times n}$ has $n$ distinct nonzero eigenvalues. Prove that there are exactly $2n$ distinct matrices $B$ such that $B^2 = A$(i.e., in particular, there are no more than $2n$ matrices with this property).
    
    \item How many such matrices $B \in \mathbb{C}^{3 \times 3}$ exist if $A = \operatorname{diag}(2, 2, 1)$?
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A$ has $n$ distinct eigenvalues, then $A$ can be unitarily diagonalizable, such that there exists unitary matrix $U$ such that $A = U \Lambda U^*$, where $\Lambda = \operatorname{diag}(\lambda_1, \cdots, \lambda_n)$ and $\lambda_i \neq 0, i = 1, \cdots, n$. Then, $U^* A U = \Lambda$.
    
    Now let $S = U^* B U$, and then $U^* B^2 U = S^2 = \Lambda$. And since $\Lambda$ is diagonal, then $S$ commutes with $\Lambda$. Suppose
    \begin{align*}
        S = \begin{pmatrix}
            s_{11} & \cdots & s_{1n} \\
            \vdots & \ddots & \vdots \\
            s_{n1} & \cdots & s_{nn}
        \end{pmatrix},
    \end{align*}
    and then $S \Lambda = \Lambda S$ implies $s_{ij} = 0$ for $i \neq j$. Indeed, $s_{ij} \lambda_j = \lambda_i s_{ij}$ and $\lambda_i \neq \lambda_j$. Therefore, $S = \operatorname{diag}(s_{11}, \cdots, s_{nn})$.
    
    Since $S^2 = \Lambda$, then $s_{ii} = \pm \sqrt{\lambda_i}$. Then, there are exactly $2^n$ choices for $S$, and thus there are exactly $2^n$ matrices $B$ such that $B^2 = A$.
    
    \item Let 
    \begin{align*}
        B = \begin{pmatrix}
            b_{11} & b_{12} & b_{13} \\
            b_{21} & b_{22} & b_{23} \\
            b_{31} & b_{31} & b_{33}
        \end{pmatrix},
    \end{align*}
    and since $B$ and $A$ commutes, then we have 
    \begin{align*}
        \begin{pmatrix}
            2b_{11} & 2b_{12} & b_{13} \\
            2b_{21} & 2b_{22} & b_{23} \\
            2b_{31} & 2b_{31} & b_{33}
        \end{pmatrix} = \begin{pmatrix}
            2b_{11} & 2b_{12} & 2b_{13} \\
            2b_{21} & 2b_{22} & 2b_{23} \\
            b_{31} & b_{31} & b_{33}
        \end{pmatrix},
    \end{align*}
    which gives 
    \begin{align*}
        B = \begin{pmatrix}
            b_{11} & b_{12} & 0 \\
            b_{21} & b_{22} & 0 \\
            0 & 0 & b_{33}
        \end{pmatrix}.
    \end{align*}
    
    Since $B^2 = A$, then we have equations
    \begin{align*}
        \begin{cases}
            b_{11}^2 + b_{12}b_{21} = 0, \\
            b_{11} b_{12} + b_{12} b_{22} = 0, \\
            b_{11} b_{21} + b_{21} b_{22} = 0, \\
            b_{22}^2 + b_{12}b_{21} = 0,
        \end{cases}
    \end{align*}
    and if $b_{22} = 0, b_{21} \neq 0$, then the equations reduces to $b_{11} = - b_{22} = 0$ and $b_{12}b_{21} = 0$, which gives infinitely choices for $b_{21}$ and $b_{12}$. Thus, in this case, there exist infinitely many matrices $B$ such that $B^2 = A = \operatorname{diag}(2, 2, 1)$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $f: \mathbb{C}^{n+1} \times \mathbb{C}^{n+1} \to \mathbb{C}$ be the function defined for all $\mathbf{x}, \mathbf{y} \in \mathbb{C}^{n+1}$ by
\begin{align*}
    f(\mathbf{x}, \mathbf{y}) = \sum^n_{j=1} x_j \overline{y}_j - x_{n+1} \overline{y}_{n+1}.
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item For $A, B \in \mathbb{C}^{(n+1) \times (n+1)}$, show that $f(A\mathbf{x}, \mathbf{y}) = f(\mathbf{x}, B\mathbf{y})$ for all $\mathbf{x}, \mathbf{y} \in \mathbb{C}^{n+1}$ if and only if $A = JB^*J$, where $J = \operatorname{diag}(1, \cdots, 1, -1)$.
    
    \item We define
    \begin{align*}
        U(n, 1) = \left\{ U \in \mathbb{C}^{(n+1) \times (n+1)} | f(U\mathbf{x}, U \mathbf{y}) = f(\mathbf{x}, \mathbf{y}), \forall \mathbf{x}, \mathbf{y} \in \mathbb{C}^{n+1} \right\}.
    \end{align*}
    Show that if $U \in U(n,1)$, then $U$ is invertible. How do $U^{-1}$ and $U^*$ relate?
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item By definition of $f$, we have
    \begin{align}\label{new_bilinear_form}
        f(\mathbf{x}, \mathbf{y}) = (\mathbf{x}, J\mathbf{y}),
    \end{align}
    where $J = \operatorname{diag}(1, \cdots, 1, -1)$. Then, since $f(A\mathbf{x}, \mathbf{y}) = f(\mathbf{x}, B\mathbf{y})$, then by the above equation, we have
    \begin{align*}
        (J^*A\mathbf{x}, \mathbf{y}) =(A\mathbf{x}, J\mathbf{y}) = (\mathbf{x}, JB\mathbf{y}) = (B^* J^* \mathbf{x}, \mathbf{y}),
    \end{align*}
    which implies $J^*A = B^* J^*$, and since $J^* = J = J^{-1}$, we have $A = J B^* J$.
    
    \item Since $f(U\mathbf{x}, U \mathbf{y}) = f(\mathbf{x}, \mathbf{y})$, then by \ref{new_bilinear_form}, we have
    \begin{align*}
        (\mathbf{x}, U^* J U\mathbf{y}) = (U\mathbf{x}, JU\mathbf{y}) = (\mathbf{x}, J\mathbf{y}),
    \end{align*}
    which implies $U^* J U = J$. Since $J = \operatorname{diag}(1, \cdots, 1, -1)$, we have
    \begin{align*}
        J U^* J U = J^2 = I, 
    \end{align*}
    and then $U^{-1} = J U^* J$. Thus, $U$ is invertible.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Prove that there exists a constant $C > 0$ such that for all matrices $A \in \mathbb{C}^{3 \times 3}$, there exists $P \in U(3)$ and a diagonal $D \in \mathbb{C}^{3 \times 3}$ such that
\begin{align*}
    \left\| P^*AP - D \right\|^2 \leq C \left\| A^* A - A A^* \right\|,
\end{align*}
where $\|B\| = \sqrt{\Tr (B B^*)}$ denotes the Hilbert-Schmidt norm of $B$.\\
{\bf Hint:} Prove the statement for the triangular matrices first.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For any upper triangular matrix $A \in \mathbb{C}^{3 \times 3}$,
    \begin{align*}
        A = \begin{pmatrix}
            a & b & c \\
            0 & d & e \\
            0 & 0 & f
        \end{pmatrix},
    \end{align*}
    then
    \begin{align*}
        A^*A - AA^* = \begin{pmatrix}
            -|b|^2 - |c|^2 & b(\overline{a} - \overline{d}) - c \overline{e} & c(\overline{a} - \overline{f}) \\
            (a - d)\overline{b} - e \overline{c} & |b|^2 - |e|^2 & e(\overline{d} - \overline{f}) + e \overline{b}\\
            (a - f)\overline{c} & (d - f) \overline{e} + b \overline{c} & |c|^2 + |e|^2
        \end{pmatrix}.
    \end{align*}
    
    Now let
    \begin{align*}
        D = \begin{pmatrix}
            0 & b & c \\
            0 & 0 & e \\
            0 & 0 & 0
        \end{pmatrix},
    \end{align*}
    then
    \begin{align*}
        \|A - D\|^2 = \Tr \begin{pmatrix}
            |b|^2 + |c|^2 & c \overline{e} & 0 \\
            e \overline{c} & |e|^2 & 0 \\
            0 & 0 & 0
         \end{pmatrix} = |b|^2 + |c|^2 + |e|^2.
    \end{align*}
    
    Also, since $A^* A - A A^*$ is Hermitian, then 
    \begin{align*}
        \sqrt{2} \left\|A^* A - A A^*\right\| & = \sqrt{2} \sqrt{\Tr \left( (A^* A - A A^*)^2 \right)} \\
        & \geq \sqrt{2} \sqrt{\left(|b|^2 + |c|^2\right)^2 + \left(|b|^2 - |e|^2\right)^2 + \left(|c|^2 + |e|^2\right)^2} \\
        & =  \sqrt{2\left(|b|^2 + |c|^2\right)^2 + 2\left(|b|^2 - |e|^2\right)^2 + 2\left(|c|^2 + |e|^2\right)^2} \\
        & \geq \sqrt{\left(|b|^2 + |c|^2\right)^2 + |b|^4 + \left(|b|^2 - |e|^2\right)^2 + \left(|c|^2 + |e|^2\right)^2 + |e|^4} \\
        & \geq \sqrt{\left(|b|^2 + |c|^2\right)^2  + \left(|b|^2 - |e|^2\right)^2 + 2|b|^2|e|^2 + \left(|c|^2 + |e|^2\right)^2} \\
        & \geq \sqrt{\left(|b|^2 + |c|^2\right)^2  + |b|^4 + |e|^4 + \left(|c|^2 + |e|^2\right)^2} \\
        & \geq \sqrt{\left(|b|^2 + |c|^2\right)^2  + 2|b|^2|e|^2 + \left(|c|^2 + |e|^2\right)^2} \\
        & = \sqrt{\left(|b|^2 + |c|^2 + |e|^2\right)^2 + |c|^4} \\
        & \geq |b|^2 + |c|^2 + |e|^2.
    \end{align*}
    Thus, 
    \begin{align*}
        \|A - D\| \leq \sqrt{2} \left\|A^* A - A A^*\right\|.
    \end{align*}
    
    \item For every matrix $A \in \mathbb{C}^{3 \times 3}$, by Schur decomposition, there exists a unitary matrix $P$ such that $P^* A P = T$, where $T$ is an upper triangular matrix. Thus, we have
    \begin{align*}
        \left\|P^* A P - D \right\|^2 & = \|T - D\|^2 \\
        & \leq \sqrt{2} \left\|T^* T - T T^*\right\| \\
        & = \sqrt{2} \left\|P^* A^* P P^* A P - P^* A P P^* A^* P\right\| \\
        & = \sqrt{2} \left\|P^* A^* A P - P^* A A^* P\right\| \\
        & = \sqrt{2} \left\|P^* (A^* A - A A^*) P\right\| \\
        & = \sqrt{2} \left\|A^* A - A A^*\right\|.
    \end{align*}
\end{enumerate}
\end{proof}


\newpage
\section{August 2017 Exam}

{\bf Some notations:} Given any linear transformation $T$, its transpose is denoted by $T^t$, and its adjoint with respect to an identified inner product is represented by $T^*$. Similarly, $A^t$ denotes the transpose of a matrix $A$, while $A^*$ denotes its adjoint, i.e. its conjugate transpose. The classical ajoint (or adjugate) of a square matrix is the transpose of the cofactor matrix $\operatorname{cof}(A)$ and is denoted by $\operatorname{adj}(A)$. A square complex matrix is said to be positive (resp. nonnegative), when $A$ is Hermitian (self-adjoint) and for all nonzero $x \in \mathbb{C}^n$, the standard inner product $(Ax, x) > 0$ (resp. nonnegative). The Hilbert-Schmidt inner product of two complex matrices $A, B \in \mathbb{C}^{n \times m}$ is defined by $A : B = \Tr \left(AB^*\right)$.

\medskip

\begin{exercise}{\rm *}
Let $A \in \mathbb{C}^{n \times n}$ be an arbitrary matrix.
\begin{enumerate}[label=(\alph*)]
    \item Prove that for all integer $m \geq 0$, $\operatorname{cof}\left(A^m\right) = \operatorname{cof}\left(A\right)^m$.
    
    \item Prove that if $n = 2$, $\operatorname{cof}\left(e^A\right) = e^{\operatorname{cof}(A)}$, where for any matrix $A \in \mathbb{C}^{n \times n}$, $e^A$ is defined by
    \begin{align*}
        e^A = \sum^\infty_{m=0} \frac{A^m}{m!}.
    \end{align*}
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item \begin{enumerate}[label=\arabic*)]
        \item If $A$ is invertible, then by $A^{-1} = \frac{1}{\det(A)} \operatorname{cof}(A)^t$, we have $\det(A) I = A \operatorname{adj}(A)$, since $\operatorname{adj}(A)$ is transpose of $\operatorname{cof}(A)$. Taking transpose on both sides gives
        \begin{align*}
            \operatorname{cof}(A) = \det(A) (A^t)^{-1}.
        \end{align*}
        Thus, we have
        \begin{align*}
            \operatorname{cof}(A^m) & = \det(A^m) [(A^m)^t]^{-1} \\
            & = \det(A)^m [(A^t)^m]^{-1} \\
            & = \det(A)^m [(A^t)^{-1}]^m = \operatorname{cof}(A)^m.
        \end{align*}
        
        \item If $A$ is not invertible. Then there exists $\varepsilon > 0$ such that $A_\varepsilon = \varepsilon I + A$ is invertible. Then, by (a), we have $\operatorname{cof}(A_\varepsilon^m) = \operatorname{cof}(A_\varepsilon)^m$. Also, since $\varepsilon I$ and $A$ commute, then $A_\varepsilon^m = \sum^{n+1}_{i=0} \binom{n}{i} (\varepsilon I)^i A^{n+1-i}$. Letting $\varepsilon \to 0$, then $\lim_{\varepsilon \to 0} A_\varepsilon = A$ and hence $\operatorname{cof}(A^m) = \operatorname{cof}(A)^m$.
    \end{enumerate}
    
    \item For any $A, B \in \mathbb{C}^{2 \times 2}$ and $c \in \mathbb{R}$, let
    \begin{align*}
        A = \begin{pmatrix}
            a_1 & a_2 \\
            a_3 & a_4
        \end{pmatrix}, \quad B = \begin{pmatrix}
            b_1 & b_2 \\
            b_3 & b_4
        \end{pmatrix},
    \end{align*}
    then we have
    \begin{align*}
        \operatorname{cof}(cA) & = \operatorname{cof}
        \begin{pmatrix}
            c a_1 & c a_2 \\
            c a_3 & c a_4
        \end{pmatrix} = \begin{pmatrix}
            c a_4 & - c a_3 \\
            - c a_32 & c a_1
        \end{pmatrix} = c \operatorname{cof}(A), \\
        \operatorname{cof}(A + B) & = \operatorname{cof} \begin{pmatrix}
            a_1 + b_1 & a_2 + b_2 \\
            a_3 + b_3 & a_4 + b_4
        \end{pmatrix} = \operatorname{cof}(A) + \operatorname{cof}(B),
    \end{align*}
    and hence $\operatorname{cof}$ is a linear operator for $A \in \mathbb{C}^{2 \times 2}$. Thus, we have
    \begin{align*}
        \operatorname{cof} \left(\sum^\infty_{m=0} \frac{A^m}{m!} \right) = \sum^\infty_{m=0} \operatorname{cof} \left(\frac{A^m}{m!}\right) = \sum^\infty_{m=0} \frac{\operatorname{cof} \left(A^m\right)}{m!} = \frac{\operatorname{cof}\left(A\right)^m}{m!} = e^{\operatorname{cof}(A)},
    \end{align*}
    where the last step comes from (a).
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}\label{Aug_2017_2}
Let $A$ and $B$ be two Hermitian complex matrices.
\begin{enumerate}[label=(\alph*)]
    \item Prove that $\Tr(AB)$ is real. 
    
    \item Prove that if $A, B$ are positive, then $\Tr(AB) > 0$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let $A = \left(a_{ij}\right)_{n \times n}$, $B = \left(b_{ij}\right)_{n \times n}$, and then $a_{ij} = \overline{a_{ji}}, b_{ij} = \overline{b_{ji}}$. Then we have
    \begin{align*}
        \Tr(AB) = \sum^n_{i=1} \sum^n_{j=1} a_{ij} b_{ji} = \sum^n_{i=1} \sum^n_{j=1} \overline{a_{ji}} \overline{b_{ij}} =  \sum^n_{i=1} \sum^n_{j=1} \overline{a_{ji} b_{ij}} = \overline{\sum^n_{i=1} \sum^n_{j=1} a_{ji} b_{ij}} = \overline{\Tr(AB)},
    \end{align*}
    and thus $\Tr(AB)$ is real. 
    
    \item Since $B$ is Hermitian, then $B$ can be diagonalized by a unitary matrix $P$ such that $B = P \Lambda P^*$, where $\Lambda$ is diagonal matrix with entries being eigenvalues of $B$. Since $B > 0$, then all eigenvalues of $B$ is positive, denoted by $\lambda_1, \cdots, \lambda_n$.
    
    Now consider $P^*AP$, since $A > 0$, then for any $x \in \mathbb{R}^n$,
    \begin{align*}
        \left(P^*APx, x\right) = \left(APx, Px\right) > 0.
    \end{align*}
    Let $x = e_i$ be the standard basis, then the above inequality implies that the diagonal entries $d_i$ of $P^*AP$ are all positive. Since similar matrix has same trace, then
    \begin{align*}
        \Tr(AB) & = \Tr\left(P^*ABP\right) = \Tr \left(P^*APP^*BP\right) \\
        & = \Tr \left(P^*AP \Lambda\right) = \sum^n_{i=1} d_i \lambda_i > 0.
    \end{align*}
\end{enumerate}
\end{proof}

\begin{proof}[Second Proof of Exercise \ref{Aug_2017_2}(b)]
Since $A$ is positive, then $A$ is invertible, and its inverse is also positive. Indeed, this is obvious if we consider the eigenvalues of $A$ and $A^{-1}$. Also, 
\begin{align*}
    AB = A^{\frac{1}{2}} A^{\frac{1}{2}} B A^{\frac{1}{2}} A^{-\frac{1}{2}},
\end{align*}
then $AB$ is similar with $A^{\frac{1}{2}} B A^{\frac{1}{2}}$. For any $x \in \mathbb{R}^n$, since $A^{\frac{1}{2}}$ is also Hermitian and $B$ is positive, then
\begin{align*}
    \left(A^{\frac{1}{2}} B A^{\frac{1}{2}}x, x\right) = \left(  B A^{\frac{1}{2}}x, \left(A^{\frac{1}{2}}\right)^* x\right) = \left(  B A^{\frac{1}{2}}x, A^{\frac{1}{2}} x\right) > 0,
\end{align*}
and hence $A^{\frac{1}{2}} B A^{\frac{1}{2}}$ is positive. Since similar matrix has the same trace, thus,
\begin{align*}
    \Tr(AB) = \Tr \left(A^{\frac{1}{2}} B A^{\frac{1}{2}} \right) > 0.
\end{align*}
\end{proof}

\medskip

\begin{exercise}
Let $A$ be an $n \times n$ matrix with $n$ distinct nonzero eigenvalues. Consider the linear map $S_A: \mathbb{C}^{n \times n} \to \mathbb{C}^{n \times n}$ given by:
\begin{align*}
    S_A(X) = AX - XA.
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item Find are the dimensions of null space and range of $S_A$.
    
    \item Find the eigenvalues and eigenvectors of $S_A$. Is $S_A$ diagonalizable?
    
    \item Find the minimal polynomial of $S_A$ when $A = \operatorname{diag}(1,2,3)$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A$ has $n$ distinct nonzero eigenvalues, then $A$ is diagonalizable, that is, there exists an invertible matrix $P$ such that $A = P \Lambda P^{-1}$, where $\Lambda$ is diagonal. Then, 
    \begin{align*}
        AX - XA & = P \Lambda P^{-1} X - X P \Lambda P^{-1} \\
        & = P \left(\Lambda P^{-1} X P - P^{-1} X P \Lambda \right) P^{-1},
    \end{align*}
    and if $X \in N_{AX - XA}$, then $\Lambda$ commutes with $P^{-1} X P$. Since $\Lambda$ is diagonal, then $P^{-1} X P$ must also be a diagonal matrix. Thus, $\dim N_{AX - XA} = n$, since we can choose a basis $\{E^i\}^n_{i=1}$ for $N_{AX - XA}$, where $E^i$ is a matrix with $E_{ii} = 1$ and $E_{ij} = 0$ otherwise. 
    
    By Rank-Nullity theorem, $\dim R_{AX - XA} = n^2 - n$. 
    
    \item Let $P^{-1} X P = B = \left(b_{ij}\right)_{n \times n}$ and $\Lambda = \operatorname{diag}(\lambda_1, \cdots, \lambda_n)$, then $AX - XA = P (\Lambda B - B \Lambda) P^{-1}$ and 
    \begin{align*}
        \Lambda B - B \Lambda = \begin{pmatrix}
            0 & (\lambda_1 - \lambda_2) b_{12} & (\lambda_1 - \lambda_3) b_{13} & \cdots & (\lambda_1 - \lambda_n) b_{1n} \\
            (\lambda_2 - \lambda_1) b_{21} & 0 & (\lambda_2 - \lambda_3) b_{23} & \cdots & (\lambda_2 - \lambda_n) b_{2n} \\
            (\lambda_3 - \lambda_1) b_{31} & (\lambda_3 - \lambda_2) b_{32} & 0 & \cdots & (\lambda_3 - \lambda_n) b_{3n} \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            (\lambda_n - \lambda_1) b_{n1} & (\lambda_n - \lambda_2) b_{n2} & (\lambda_n - \lambda_3) b_{n3} & \cdots & 0
        \end{pmatrix}.
    \end{align*}
    Consider $X$ such that $S_A(X) = \lambda X$ for some $\lambda$, then it is equivalent to that $AX - XA = P (\Lambda B - B \Lambda) P^{-1} = P (\lambda B) P^{-1}$. It suffice to find $X$ such that $\Lambda B - B \Lambda = \lambda B$. Then, we can choose $B$ as $E^{ij}$, where $E^{ij}$ is a matrix where the entry on $i$th row and $j$th column of $E^{ij}$ is $1$, and all other entries are zero. Now we have
    \begin{align*}
        S_A(E^{ij}) = \begin{cases}
            (\lambda_i - \lambda_j) E^{ij}, & i \neq j, \\
            0, & i = j.
        \end{cases}
    \end{align*}
    Then, $\left\{P E^{ij} P^{-1}\right\}_{i,j=1}$ form a basis for eigenvectors of $S_A$, and eigenvalues are $0, \lambda_i - \lambda_j$ for all $i \neq j$. And since $\{P E^{ij} P^{-1}\}_{i,j=1}$ also forms a basis for $\mathbb{C}^{n \times n}$, $S_A$ is diagonalizable.
    
    \item Since $A = \operatorname{diag}(1,2,3)$, then by (b), the eigenvlaues for $S_A$ are $0, \pm 1, \pm 2$. Since $S_A$ is diagoonalizable, then the minimal polynomial is 
    \begin{align*}
        m_{S_A}(\lambda) = \lambda \left(\lambda^2 - 1\right) \left(\lambda^2 - 4\right).
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $V$ be a real inner product vector space of finite dimension and let $W \subset V$ be a linear subspace. Suppose that $S$ is a positive operator on $V$. Let $P$ be the orthogonal projection on $W$. Prove that $PS$ is diagonalizable.
\end{exercise}
\begin{proof}
Since $P$ is orthogonal projection, then for any $x \in V$, $x$ can be written as $x = Px + x^\bot$, where $Px \in W$ and $x_\bot \in W^\bot$. Then, for any $x, y \in V$,
\begin{align*}
    (Px, y - Py) = 0 = (x - Px, Py),
\end{align*}
which implies 
\begin{align*}
    (Px, y) = (Px, Py) = (x, Py),
\end{align*}
and hence $P = P^*$.

Now, since $S$ is positive and
\begin{align*}
    PS = S^{-\frac{1}{2}} S^{\frac{1}{2}} P S^{\frac{1}{2}} S^{\frac{1}{2}},
\end{align*}
then $PS$ is similar to $S^{\frac{1}{2}} P S^{\frac{1}{2}}$. Also,
\begin{align*}
    \left(S^{\frac{1}{2}} P S^{\frac{1}{2}} \right)^* = S^{\frac{1}{2}} P S^{\frac{1}{2}},
\end{align*}
then $S^{\frac{1}{2}} P S^{\frac{1}{2}}$ is symmetric and then diagonalizable. Thus, $PS$ is also diagonalizable.
\end{proof}

\medskip

\begin{exercise}
Let $R \in O(4)$.
\begin{enumerate}[label=(\alph*)]
    \item Prove that $R$ admits an invariant two dimensional subspace $W$.
    
    \item Prove that if $\det R > 0$, then it is similar to a matrix of the form 
    \begin{align*}
        \begin{pmatrix}
            a & b & 0 & 0 \\
            -b & a & 0 & 0\\
            0 & 0 & c & d \\
            0 & 0 & -d & c\\ 
        \end{pmatrix},
    \end{align*}
    with $a, b, c, d \in \mathbb{R}$ and $a^2 + b^2 = c^2 + d^2 = 1$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $R \in O(4) \subset {\rm GL}(4, \mathbb{R})$, then $RR^T = R^T R = I$ and hence $R$ is unitarily diagonalizable\footnote{Normal matrix is unitarily diagonalizable.}. Suppose the characteristic polynomial for $R$ is 
    \begin{align*}
        p_R(\lambda) = (\lambda - \lambda_1)^{d_1} \cdots (\lambda - \lambda_4)^{d_4},
    \end{align*}
    where $d_i \in \{0, 1, 2, 3, 4\}$ and $\sum^4_{i=1} d_i = 4$. Also, since $R$ is diagonalizable, then 
    \begin{align*}
        \dim N_{R-\lambda_i I} = d_i, i = 1,2,3,4
    \end{align*}
    Then, no matter how many eigenvalues $R$ has, $R$ always admits an invariant subspace of dimension $2$. Indeed,
    \begin{enumerate}[label=\arabic*)]
        \item If $p_R = \prod^4_{i=1} (\lambda - \lambda_i)$, then each eigenvalue has an eigenvector $x_i, i = 1,2,3,4$. Choose any two vectors $x_j, x_k \in \{x_i\}^4_{i=1}$ and then $\operatorname{span}\{x_j, x_k\}$ is an invariant subspace.
        
        \item If $p_R = (\lambda - \lambda_1)(\lambda - \lambda_2)(\lambda - \lambda_3)^2$, then $\dim N_{\lambda - \lambda_3 I} = 2$, which is an invariant subspace. 
        
        \item For any higher multiplicity for eigenvalue, it is similar to 2) and we can always find an invariant subspace.
    \end{enumerate}
    
    \item For any eigenvalue $\lambda$ of $R$ and its eigenvector $x$, we have
    \begin{align*}
        (Rx, Rx) = \lambda^2 (x, x) = (R^*Rx, x) = (x,x),
    \end{align*}
    which implies $|\lambda| = 1$.
    \begin{enumerate}[label=\arabic*)]
        \item If all $\lambda \in \mathbb{R}$, then the number of eigenvalue $-1$ is even. Then, let $b = d = 0$, then $R$ is similar to 
        \begin{align*}
            \begin{pmatrix}
                a & 0 & 0 & 0 \\
                0 & a & 0 & 0\\
                0 & 0 & c & 0 \\
                0 & 0 & 0 & c\\ 
            \end{pmatrix},
        \end{align*}
        where $a = \pm 1$ and $c = \pm 1$.
        
        \item If all $\lambda \notin \mathbb{R}$, suppose $\lambda = a + ib$, then $a - ib$ is also an eigenvalue. Similarly, assume another two eigenvalues are $c \pm id$. Then, under certain basis $\mathcal{B}$,
        \begin{align*}
            [R]_{\mathcal{B}} = \begin{pmatrix}
                a & b & 0 & 0 \\
                -b & a & 0 & 0\\
                0 & 0 & c & d \\
                0 & 0 & -d & c\\ 
            \end{pmatrix},
        \end{align*}
        and $a^2 + b^2 = c^2 + d^2 = 1$, since $\det R = 1$.
        
        \item If $\lambda_1 \in \mathbb{R}$ and $\lambda_2 \notin \mathbb{R}$, then let $d = 0$ and choose certain certain basis $\mathcal{B}$,
        \begin{align*}
            [R]_{\mathcal{B}} = \begin{pmatrix}
                a & b & 0 & 0 \\
                -b & a & 0 & 0\\
                0 & 0 & c & 0 \\
                0 & 0 & 0 & c\\ 
            \end{pmatrix},
        \end{align*}
        where $a^2 + b^2 = 1$ and $c = \pm 1$.
    \end{enumerate}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}\label{Aug_2017_6}
Prove that for all $n$, $\operatorname{span} {\rm U}(n) = \mathbb{C}^{n \times n}$, where ${\rm U}(n)$ is the group of all $n \times n$ unitary matrices.\\
{\bf Hint:} Polar decomposition theorem could be useful.
\end{exercise}
\begin{proof}
First, we could prove that any Hermitian matrix is a linear combination of unitary matrices. Then, by Polar decomposition, the result follows\cite{8}.
\begin{enumerate}[label=(\alph*)]
    \item For any Hermitian matrix $R$, it can be digonalized, that is, there is a unitary matrix $P$ such that $R = P \Lambda P^*$, where $\Lambda$ is diagonal. We want to show that any Hermitiain is a linear combination of unitary matrices. 
    
    Suppose $\Lambda = \operatorname{diag}(\lambda_1, \cdots, \lambda_n)$, then let $\widetilde{\Lambda} = \operatorname{diag}(\lambda_1/\lambda, \cdots, \lambda_n/\lambda)$, where $\lambda = \max_{i}|\lambda_i|$. Now, let $U_A$ be a diagonal matrix with diagonal entries $\lambda_i/\lambda + i \sqrt{1 - (\lambda_i/\lambda)^2}, i = 1, \cdots, n$ and $U_B$ be a diagonal matrix with diagonal entries $\lambda_i/\lambda - i \sqrt{1 - (\lambda_i/\lambda)^2}, i = 1, \cdots, n$. Then, $U_A$ and $U_B$ are unitary matrices, since
    \begin{align*}
        \left(\frac{\lambda_i}{\lambda} + i \sqrt{1 - \left(\frac{\lambda_i}{\lambda}\right)^2} \right) \left(\frac{\lambda_i}{\lambda} - i \sqrt{1 - \left(\frac{\lambda_i}{\lambda}\right)^2} \right) = 1.
    \end{align*}
    Also, $\Lambda/\lambda = \widetilde{\Lambda} = U_A + U_B$, then we have $\Lambda = \lambda (U_A + U_B)$. Thus, any Hermitiain is a linear combination of unitary matrices.
    
    \item By Polar decomposition, any matrix $A$ can be factores as $A = RU$, where $R$ is nonnegative self-adjoint matrix and $U$ is a unitary matrix. Now (a), $R = \lambda P (U_A + U_B) P^*$, and then $A = \lambda P (U_A + U_B) P^*U$, where $P, U_A, U_B, U$ are all unitary. Since if $A, B$ are unitary, then $AB$ is also unitary. Thus, $A$ can be written as a linear combination of unitary matrices.
\end{enumerate}
Thus, we have $\operatorname{span} {\rm U}(n) = \mathbb{C}^{n \times n}$.
\end{proof}

\begin{proof}[Second Proof of Exercise \ref{Aug_2017_6}(b)]
For any matrix $A \in \mathbb{C}^{n \times n}$, let 
\begin{align*}
    A^+ = \frac{A + A^*}{2}, \quad A^{-} = \frac{A - A^*}{2i},
\end{align*}
then $A = A^+ + i A^{-}$. Also, $A^+$ and $A^{-}$ are all Hermitian. Then, by (a), $A$ can also be written as a linear combination of unitary matrices.
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{Aug_2017_6}]
Let $(,)$ be the Hilbert-Schmidt inner product on $\mathbb{C}^{n \times n}$, i.e. for $A, B \in \mathbb{C}^{n \times n}$, 
\begin{align*}
    A:B = \Tr\left(A^*B\right).
\end{align*}
Let $W = \operatorname{span} {\rm U}(n)$, then $W$ is a subspace of $\mathbb{C}^{n \times n}$. Then, we have
\begin{align*}
    W \oplus W^{\bot} = \mathbb{C}^{n \times n},
\end{align*}
where $W^{\bot} = \{T \in \mathbb{C}^{n \times n} | T:U = 0, U \in W\}$ is the annihilator of $W$. For any $T \in W^{\bot}$, by Polar decomposition, there exists a nonnegative symmetric matrix $R$ and a unitary matrix $U$ such that $T = UR$\cite{9}. Then,
\begin{align*}
    0 = T:U = UR:U = \Tr\left(R^*U^*U\right) = \Tr(R),
\end{align*}
which implies $R = 0$. Then, $T = 0$, i.e. $W^\bot = \{0\}$. Thus, $\operatorname{span} {\rm U}(n) = \mathbb{C}^{n \times n}$.
\end{proof}


\newpage
\section{April 2017 Exam}

{\bf Some notations:} Given any linear transformation $T$, its transpose is denoted by $T^t$, and its adjoint with respect to an identified inner product is represented by $T^*$. Similarly, $A^t$ denotes the transpose of a matrix $A$, while $A^*$ denotes its adjoint, i.e. its conjugate transpose. The classical ajoint (or adjugate) of a square matrix is the transpose of the cofactor matrix $\operatorname{cof}(A)$ and is denoted by $\operatorname{adj}(A)$. A square complex matrix is said to be positive (resp. nonnegative), when $A$ is Hermitian (self-adjoint) and for all nonzero $x \in \mathbb{C}^n$, the standard inner product $(Ax, x) > 0$ (resp. nonnegative). The Hilbert-Schmidt inner product of two complex matrices $A, B \in \mathbb{C}^{n \times m}$ is defined by $A : B = \Tr \left(AB^*\right)$.

\medskip

\begin{exercise}
Assume that $A = \left[a_{jk}\right] \in \mathbb{C}^{n \times n}$ is a nonnegative rank $1$ matrix . Prove that there exists a nonzero $v = (v_1, \cdots, v_n) \in \mathbb{C}^{n}$ such that
\begin{align*}
    a_{jk} = v_j \overline{v_k}, \quad \forall j,k \in \{1, \cdots, n\}.
\end{align*}
\end{exercise}
\begin{proof}
Since $A$ is nonnegative matrix, then it is a Hermitian matrix, then hence it can be unitarily diagonalized, that is, there exists a unitary matrix $P$ such that $A = P \Lambda P^*$, where $\Lambda$ is diagonal. Also, since $A$ is rank $1$, then there is only one diagonal entry $d$ in $\Lambda$, and $d \geq 0$. Suppose, $d$ appears on $i$th row, then
\begin{align*}
    A & = P \Lambda P^* \\
    & = \begin{pmatrix}
        p_{11} & \cdots & p_{1n} \\
        \vdots & \ddots & \vdots \\
        p_{n1} & \cdots & p_{nn}
    \end{pmatrix}
    \begin{pmatrix}
        0 &  &  &  & \\
        & \ddots &  &  & \\
        &  & d &  & \\ 
        &  &  & \ddots & \\
        &  &  &  & 0 
    \end{pmatrix}
    \begin{pmatrix}
        \overline{p_{11}} & \cdots & \overline{p_{n1}} \\
        \vdots & \ddots & \vdots \\
        \overline{p_{1n}} & \cdots & \overline{p_{nn}}
    \end{pmatrix} \\
    & = \begin{pmatrix}
        p_{11} & \cdots & p_{1n} \\
        \vdots & \ddots & \vdots \\
        p_{n1} & \cdots & p_{nn}
    \end{pmatrix} 
    \begin{pmatrix}
        0 & 0 & \cdots & 0 \\
        \vdots & \vdots &  & \vdots \\
        d\overline{p_{1i}} & d\overline{p_{2i}} & \cdots & d\overline{p_{ni}} \\
        \vdots & \vdots &  & \vdots \\
        0 & 0 & \cdots & 0
    \end{pmatrix} \\
    & = \begin{pmatrix}
        d p_{1i}\overline{p_{1i}} & \cdots & d p_{1i} \overline{p_{ni}} \\
        \vdots & \ddots & \vdots \\
        d p_{ni}\overline{p_{1i}} & \cdots & d p_{ni} \overline{p_{ni}}
    \end{pmatrix}.
\end{align*}
Let $v_j = \sqrt{d}p_{ji}$ for $j = 1, \cdots, n$, then we have 
\begin{align*}
    a_{jk} = d p_{ji}\overline{p_{ki}} = v_i \overline{v_k}.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $O(n)$ be the set of orthogonal matrices in $\mathbb{R}^{n \times n}$. We define the set of special orthogonal matrices by
\begin{align*}
    SO(n) = \left\{R \in O(n) \subset \mathbb{R}^{n \times n} | \det R = 1 \right\}.
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item Let $W = \operatorname{span} SO(2)$. Find a basis for $W$.
    
    \item Find $W^\bot$ with respect to the Hilbert-Schmidt inner product on $\mathbb{R}^{2 \times 2}$.
    
    \item Prove that for any matrix $A \in \mathbb{R}^{2 \times 2}$, there exist orthogonal matrices $R_1, R_2 \in O(2)$ and nonnegative scalars $\alpha_1, \alpha_2 \in \mathbb{R}$ such that
    \begin{align*}
        A = \alpha_1 R_1 + \alpha_2 R_2.
    \end{align*}
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let $A \in W$ and set $A = \begin{pmatrix} a & b \\ c & d
    \end{pmatrix}$, then we have following equations
    \begin{align*}
        \begin{cases}
            ac + bd = 0, \\
            ad - bc = 1, \\
            a^2 + b^2 = c^2 + d^2 = 1,
        \end{cases}
    \end{align*}
    which implies that
    \begin{align*}
        b = \pm \sqrt{1 - a^2}, \quad a = d, \quad c = -b.
    \end{align*}
    Now we can choose
    \begin{align*}
        B_1 = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix},\quad 
        B_2 = \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix},
    \end{align*}
    and then $B_1$ is linearly independent with $B_2$. For any $A \in W$, $A$ has the form of
    \begin{align*}
        \begin{pmatrix} 
            a & \pm \sqrt{1 - a^2} \\ 
            \mp \sqrt{1 - a^2} & a
        \end{pmatrix} = a B_1 \pm \sqrt{1 - a^2} B_2,
    \end{align*}
    which implies that $\{B_1, B_2\}$ is a basis for $W$.
    
    \item If $A \in W^\bot$, then $\Tr(AE_i^*) = 0, i = 1, 2$. Let $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, then 
    \begin{align*}
        \Tr\left(AE_1^*\right) & = a + d = 0, \\
        \Tr\left(AE_2^*\right) & = c - b = 0.
    \end{align*}
    Thus, we have
    \begin{align*}
        W^\bot = \left\{\begin{pmatrix} a & b \\ b & -a \end{pmatrix} \Bigg| \,\, a, b \in \mathbb{R} \right\}.
    \end{align*}
    
    \item From (a), we have
    \begin{align*}
        W = \left\{\begin{pmatrix} a & b \\ -b & a \end{pmatrix} \Bigg| \,\, a, b \in \mathbb{R} \right\}.
    \end{align*}
    Then, if $A \in W \cap W^\bot$, then $A = 0$, which implies $W \cap W^\bot = \{0\}$. Since $\dim W + \dim W^\bot = 2$, then we have $\mathbb{R}^{2 \times 2} = W \oplus W^\bot$.
    
    Then, for any $A \in \mathbb{R}^{2 \times 2}$, there exists $W_1 \in W$ and $W_2 \in W^\bot$ such that $A = W_1 + W_2$. Let $W_1 = \begin{pmatrix} x_1 & y_1 \\ y_1 & -x_1 \end{pmatrix}$ and $W_2 = \begin{pmatrix} x_2 & y_2 \\ y_2 & -x_2 \end{pmatrix}$, then $\frac{W_1}{\sqrt{x_1^2 + y_1^2}}$ and $\frac{W_2}{\sqrt{x_2^2 + y_2^2}}$ are an orthogonal matrix, denoted by $R_1, R_2$. Thus, we have
    \begin{align*}
        A = \alpha_1 R_1 + \alpha_2 R_2,
    \end{align*}
    where $\alpha_1 = \sqrt{x_1^2 + y_1^2}, \alpha_2 = \sqrt{x_2^2 + y_2^2}$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
For a Hermitian matrix $A \in \mathbb{C}^{n \times n}$, let $\lambda_j(A)$ denote the $j$-th eigenvalue of $A$ in increasing order, i.e. $\lambda_1(A) \leq \cdots \leq \lambda_n(A)$. Let $A$ and $B$ be two Hermitian matrices. Use the min-max principle to prove that if $j + k > n$, then
\begin{align*}
    \lambda_{j+k-n} (A + B) \leq \lambda_j(A) + \lambda_k(B).
\end{align*}
\end{exercise}
\begin{proof}
By min-max principle, there exists $W \subset \mathbb{C}^n, \dim = j$ and $U \subset \mathbb{C}^n, \dim = k$ such that
\begin{align*}
    \lambda_j(A) = \max_{x \in W} \frac{(Ax, x)}{(x,x)}, \quad \lambda_k(A) = \max_{x \in U} \frac{(Bx, x)}{(x,x)}
\end{align*}
Also, since $W + U \subset \mathbb{C}^n$, then
\begin{align*}
    \dim (W \cap U) = \dim W + \dim U - \dim (W + U) \geq j + k - n > 0.
\end{align*}
Then there exists a subspace $S$ of $W \cap U$ such that $\dim S = j + k - n$. For any $y \in S$, we have
\begin{align*}
    \frac{((A+B)y, y)}{(y, y)} & = \frac{(Ay, y)}{(y, y)} + \frac{(By, y)}{(y, y)} \\
    & \leq \max_{x \in W} \frac{(Ax, x)}{(x, x)} + \max_{x \in U} \frac{(Bx, x)}{(x, x)} \\
    & = \lambda_j(A) + \lambda_k(B),
\end{align*}
and thus, we have
\begin{align*}
    \lambda_{j+k-n} (A + B) & = \min_{\dim Q=j+k-n} \max_{x\in Q} \frac{((A+B)x,x)}{(x,x)} \\
    & \leq \max_{x\in S} \frac{((A+B)x,x)}{(x,x)} \\
    & \leq \max_{x \in W} \frac{(Ax, x)}{(x, x)} + \max_{x \in U} \frac{(Bx, x)}{(x, x)} \\
    & = \lambda_j(A) + \lambda_k(B).
\end{align*}
\end{proof}

\medskip

\begin{exercise}\label{April_2017_4}
Let $V_n$ be the vector space of polynomials in $\mathbb{C}[x]$ of degree less than or equal to $n$. Let the operator $T: V_n \to V_n$ be defined by $T(p) = p - p'$, where $p'$ denotes the derivative of $p$.
\begin{enumerate}[label=(\alph*)]
    \item Prove that $T$ is invertible and show that $T^{-1}$ is a polynomial in $T$.
    
    \item{\rm *} Find the Jordan form of $T^{-1}$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For $p = a_0 + a_1x + \cdots + a_nx^n$, we have
    \begin{align*}
        T(p) = (a_0 - a_1) + (a_1 - a_2)x + (a_2 - 2a_3)x^2 + \cdots + (a_{n-1} - na_n)x^{n-1} + a_n x^n.
    \end{align*}
    Then, under basis $\mathcal{B} = \{1, x, \cdots, x^n\}$, 
    \begin{align*}
        [T]_{\mathcal{B}} = \begin{pmatrix}
            1 & -1 & 0 & \cdots & 0 \\
            0 & 1 & -2 & \cdots & 0 \\
            0 & \ddots & 1 & \ddots & \vdots \\
            \vdots & \vdots & \ddots & \ddots & -n \\
            0 & \cdots & \cdots & 0 & 1
        \end{pmatrix}.
    \end{align*}
    Since $\det [T]_{\mathcal{B}} = 1$, then $T$ is invertible. 
    
    Also, the characteristic polynomial for $[T]_{\mathcal{B}}$ is $p_T(\lambda) = (\lambda - 1)^{n+1}$, then by Cayley-Hamilton theorem, $(T - I)^{n+1} = 0$, which is equivalent to 
    \begin{align*}
       \sum^{n+1}_{i=0}\binom{n+1}{i} T^{i} (-1)^{n+1-i} I^{n+1-i} = 0.
    \end{align*}
    Since $T$ is invertible, thus we have
    \begin{align*}
        T^{-1} = \sum^{n+1}_{i=0}\binom{n+1}{i} (-1)^{n+1-i} T^{i-1},
    \end{align*}
    which is a polynomial of $T$.
    
    \item Since $(T - I)^{n+1} = 0$ and $(T - I)^{n} \neq 0$, then the minimal polynomial of $T$ is $m_T(\lambda) = (\lambda - 1)^{n+1} = 0$. Then $T$ has an eigenvector $v$ such that $\left\{v, Tv, T^2v, \cdots, T^nv \right\}$ form a basis for $V_n$. 
    
    Also, $(T^{-1})^n$ is invertible, then
    \begin{align*}
        \left\{(T^{-1})^nv, (T^{-1})^nTv, (T^{-1})^nT^2v, \cdots, (T^{-1})^nT^nv \right\} = \left\{v, (T^{-1})v, (T^{-1})^2v, \cdots, (T^{-1})^nv \right\}
    \end{align*}
    also form a basis for $V_n$. Then, the charasteristic polynomial and minimal polynomial of $T^{-1}$ are the same. Also, for any eigenvector $v$ of $T$, we have $Tv = v$, which imlplies $T^{-1}v = v$. And then $1$ is also eigenvalue of $T^{-1}$. Then $T^{-1}$ has the charasteristic polynomial as $T$, which is $(\lambda - 1)^{n+1}$. Thus, the Jordan form of $T^{-1}$ is 
    \begin{align*}
        J_{T^{-1}} = \begin{pmatrix}
            1 & 1 &  &  & \\
              & 1 & 1 &  & \\
              &   & \ddots & \ddots \\
              &   &        &  1 & 1 \\
              &   &        &    & 1
        \end{pmatrix}.
    \end{align*}
\end{enumerate}
\end{proof}

\begin{proof}[Second Proof of Exercise \ref{April_2017_4}(b)]
Since $(T - I)^n \neq 0$, then the minimal polynomial for $T$ is $m_T = (\lambda - 1)^{n+1}$.

Let $N = T^{-1} - I$. Since $T$ is invertible, then $(T - I)^{n+1} = 0$ implies $T^{n+1}(I - T^{-1})^{n+1} = 0$, which is equivalent to that $(I - T^{-1})^{n+1} = $. Then $N^{n+1} = 0$, and hence the minimal polynomial of $T^{-1}$ is $m_{T^{-1}} = (\lambda - 1)^{n+1} = 0$. Thus, the Jordan form of $T^{-1}$ is 
\begin{align*}
    J_{T^{-1}} = \begin{pmatrix}
        1 & 1 &  &  & \\
        & 1 & 1 &  & \\
        &   & \ddots & \ddots \\
        &   &        &  1 & 1 \\
        &   &        &    & 1
    \end{pmatrix}.
\end{align*}
\end{proof}

\medskip

\begin{exercise}\label{April_2017_5}
Assume that $V$ is a finite dimensional inner product space and that $T \in \mathscr{L}(V, V)$. Prove that $T$ is normal if and only if there exists a unitary operator $U \in \mathscr{L}(V, V)$ such that $T^* = UT$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item ($\Rightarrow$) By the Polar decomposition, then there exists a unitary operator $U$ and a nonnegative Hermitiain matrix $R$ such that $T = UR$. Since $T$ is normal, then $TT^* = T^*T$ implies
    \begin{align*}
        R R^* = R^2 = U R^* R U^*= (U R U^*)^2.
    \end{align*}
    Since $R$ is nonnegative, then it has unique square root, and hence $R = U R U^*$. Then, $T^* = R^* U^* = R U^* = U^* R = (U^*)^2 T$. Let $\widetilde{U} = (U^*)^2$, then it is obvious that $\widetilde{U}$ is also unitray, and thus, $T^* = \widetilde{U} T$.
    
    \item ($\Leftarrow$) If $T^* = UT$, where $U$ is a unitary matrix. Then, $TT^* = (UT)^* UT = T^* T$, and thus $T$ is normal.
\end{enumerate}
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{April_2017_5}(a)]{\rm *}
Since $T$ is normal, then for any $v \in V$,
\begin{align*}
    \|Tv\|^2 = (Tv, Tv) = \left(v, T^*Tv\right) = \left(v, TT^*v\right) = \left(T^*v, T^*v\right) = \|T^*v\|^2,
\end{align*}
which implies $N_T = N_{T^*}$. 

For any $w \in R_T$, there exists $x \in V$ such that $Tx = w$. For $v \in N_{T^*}$, we have
\begin{align*}
    (w, v) = (Tx, v) = (x, T^*v) = 0,
\end{align*}
which implies $N_{T^*} \subset R_T^\bot$. On the other hand, for any $y \in R_T^\bot$ and any $v \in V$, we have $Tv \in R_T$ and then
\begin{align*}
    0 = (y, Tv) = (T^*y, v).
\end{align*}
Then $T^*y = 0$, which implies $y \in N_{T^*}$ and hence, $R_T^\bot \subset N_{T^*}$. Thus, $R_T^\bot = N_{T^*}$.

By $V = R_T \oplus R_T^\bot$, we have $V = R_T \oplus N_{T^*}$. Then for any $v \in V$, there exists unique $x \in V$ and $y \in N_{T^*}$ such that $v = Tx + y$, where $Tx \in R_T$.

Now we define $U \in \mathscr{L}(V, V)$ by
\begin{align*}
    Uv = T^* x + y,
\end{align*}
then for any $v \in V$, since $Tv = Tv + 0$, then we have
\begin{align*}
    UTv = T^*v + 0 = T^*v,
\end{align*}
that is $T^* = UT$.

It remains to show that $U$ is unitary. For any $v \in V$, $v$ can be written as $v = Tx + y$, $y \in N_{T^*}$. Then,
\begin{align*}
    (Uv, Uv) & = (T^* x + y, T^* x + y) \\
    & = (T^*x, T^*x) + (T^*x, y) + (y, T^*x) + (y, y) \\
    & = (T^*x, T^*x) + (y, y) \\
    & = (x, TT^*x) + (y,y) \\
    & = (x, T^*Tx) + (y,y) \\
    & = (Tx, Tx) + (y,y) \\
    & = (Tx, Tx) + (Tx, y) + (y, Tx) + (y,y) \\
    & = (Tx + y, Tx + y) = (v, v),
\end{align*}
and thus $U$ is unitary.
\end{proof}

\medskip

\begin{proof}[Third Possible Proof of Exercise \ref{April_2017_5}(a)]
If $T$ is normal, then $T$ can be unitarily diagonalizable, that is, there exists a unitary matrix $P$ such that $T = PDP^*$, where $D$ is diagonal. 
\begin{enumerate}[label=\arabic*)]
    \item If eigenvalues of $T$ are all real numbers, then $D$ is diagonal matrix with real diagonal entries, and hence $T^* = PD^*P^* = T$. Let $U = I$ be a unitary matrix, then $T^* = UT$.
    
    \item If eigenvalues of $T$ has complex numbers, denoted by $a + ib$, then there is another eigenvalue $a - ib$. For eigenvalues $a \pm ib$, let matrix $\widetilde{T} = \begin{pmatrix} a+ib & 0 \\ 0 & a-ib \end{pmatrix}$ and $\widetilde{U} = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$, then we have 
    \begin{align*}
        \widetilde{U} \widetilde{T} \widetilde{U}^* = \widetilde{T}^* = \begin{pmatrix} a-ib & 0 \\ 0 & a+ib \end{pmatrix}
    \end{align*}
    Then, for complex eigenvalues, we could choose $2 \times 2$ matrix $\widetilde{U}$ in diagonal block in $U$, and for real eigenvalues, we could choose $1$ in diagonal entry in $U$. Thus, we have $D^* = U D U^*$. From here, I have no ideas right now.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Suppose $A \in \mathbb{R}^{n \times n}$ is anti-symmetric, i.e. $A^T = - A$ and $B \in \mathbb{R}^{n \times n}$ is symmtric, i.e. $B^T = B$.
\begin{enumerate}[label=(\alph*)]
    \item Prove that if $n$ is even, then $\operatorname{cof}(A):B = 0$.
    
    \item Prove that if $n$ is even and $B$ is of rank $1$, then $\det(A + B) = \det A$. \\
    {\bf Hint:} You can use that the determinant function is $n$-linear in matrix rows.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For $A \in \mathbb{R}^{n \times n}$ and $n$ is even, then we have
    \begin{align*}
        \operatorname{cof}(A)_{ij} = - \operatorname{cof}(A)_{ij}.
    \end{align*}
    Indeed, observing simple $4 \times 4$ anti-symmetric matrix would gives us this result. Then, $\operatorname{cof}(A)^T = - \operatorname{cof}(A)$. Then, for symmetric matrix $B \in \mathbb{R}^{n \times n}$, then we have
    \begin{align*}
        \Tr(\operatorname{cof}(A) B^*) = \Tr(B \operatorname{cof}(A)^T) = \Tr(- B \operatorname{cof}(A)^T) = - \Tr(\operatorname{cof}(A) B^*),
    \end{align*}
    which implies $\operatorname{cof}(A):B = \Tr(\operatorname{cof}(A) B^*) = 0$.
    
    \item Since $B$ is symmetric, then $B$ is unitarily diagonalizable, that is, there exists a unitary matrix $U$ such that $B = U \Lambda U^*$, where $\Lambda$ is diagonal. Also, since $\operatorname{rank}(B) = 1$, then there is only one diagonal entry in $\Lambda$, denoted by $\lambda_1$.
    
    Now, let $\widetilde{A} = U^*AU$, then
    \begin{align*}
        \det(A + B) & = \det\left(U^*(A + B)U\right) \\
        & = \det \left(U^*AU + \Lambda \right) \\
        & = \det (\widetilde{A} + \Lambda),
    \end{align*}
    where
    \begin{align*}
        \Lambda = \begin{pmatrix} 
            \lambda_1 &  &  &  \\
            & 0 &  &  \\
            &  & \ddots & \\
            &  &  & 0
        \end{pmatrix},
    \end{align*}
    and $K$ is also anit-symmetric. Also, $\widetilde{A} + \Lambda$ has the form 
    \begin{align*}
        \begin{pmatrix} 
            \lambda_1 & a_{12} & \cdots & a_{1n} \\
            -a_{12} & 0 & \cdots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            -a_{1n} & -a_{2n} & \cdots & 0
        \end{pmatrix},
    \end{align*}
    then we have
    \begin{align*}
        \det(A + B) & = \det (\widetilde{A} + \Lambda) \\
        & = \lambda_1 \operatorname{cof}(\widetilde{A} + \Lambda)_{11} + \det \widetilde{A} \\
        & = 0 + \det \widetilde{A} = \det A.
    \end{align*}
    Indeed, from (a), we could know that for $n \times n$ anti-symmetric matrix $A$, $\operatorname{cof}(A)_{ii} = 0$, and $\operatorname{cof}(\widetilde{A} + \Lambda)_{11} = \operatorname{cof}(\widetilde{A})$.
\end{enumerate}
\end{proof}


\newpage
\section{August 2016 Exam}

\begin{exercise}{\rm *}
Consider the vector space ${\rm M}_{n\times n}(\mathbb{R})$ of $n \times n$ matrices. Consider the linear map $T: {\rm M}_{n\times n}(\mathbb{R}) \to {\rm M}_{n\times n}(\mathbb{R})$ given by $T(A) = A^T$ for all $A \in {\rm M}_{n\times n}(\mathbb{R})$. Here $A^T$ denotes the transpose of $A$.
\begin{enumerate}[label=(\alph*)]
    \item Find the characteristic polynomial and minimal polynomial of $T$.
    
    \item Find the Jordan form of $T$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $T(A) = A^T$, then $T^2(A) = A$, which implies $T^2 = I$. Then the polynomial $\lambda^2 - 1$ annihilates $T$ \footnote{We cannot assume that $\lambda^2 - 1$ is the characteristic polynomial yet.}. Also, since $T \neq \pm I$, then $\lambda - 1$ and $\lambda + 1$ is not minimal polynomial. Thus, the minimal polynomial of $T$ is $m_T(\lambda) = \lambda^2 - 1$.
    
    Consider the space $W$ of $n \times n$ real symmetric matrices, then $T(A) = A$ for $A \in W$. Since $\dim W = n(n+1)/2$, then $T$ has eigenvalue $1$ with multiplicity $n(n+1)/2$. 
    
    Now consider the space $V$ of $n \times n$ real skew-symmetric matrices\footnote{By direct-sum decomposition, any matrix can be written as a sum of a symmetric matrix and a skew-symmetric matrices, that is $A = (A + A^T)/2 + (A - A^T)/2$.}, then $T(A) = -A$ for $A \in V$. Since $\dim V = n(n-1)/2$, then $T$ has eigenvalue $-1$ with multiplicity $n(n-1)/2$. 
    
    Thus, the characteristic polynomial of $T$ is 
    \begin{align*}
        p_T(\lambda) = (\lambda - 1)^{\frac{n(n+1)}{2}} (\lambda + 1)^{\frac{n(n-1)}{2}}.
    \end{align*}
    
    \item Now, since $W \cap V = \{0\}$ and $\dim W + \dim V = n^2$, then $W \oplus V = \mathbb{R}^{n^2}$. Then, $T$ is diagonalizable\footnote{Theorem 2 in Chapter 6, \textit{Linear Algebra}, Hoffman, K. and Kunze, R.A.}. Then, $\dim N_{T - I} = n(n+1)/2$ and $\dim N_{T + I} = n(n-1)/2$, which implies that the Jordan form of $T$ is
    \begin{align*}
        J_T = \begin{pmatrix}
            1 & & & & & \\
            & \ddots & & & & \\
            & & 1 & & & \\
            & & & -1 & & \\
            & & & & \ddots & \\
            & & & & & -1
        \end{pmatrix},
    \end{align*}
    where $1$ appears $n(n+1)/2$ times and $-1$ appears $n(n-1)/2$ times.
\end{enumerate}
\end{proof}

\medskip

\newpage
\begin{exercise}\label{August_2016_2}
Let $V$ be a finite dimensional vector space and $A: V \to V$ a linear map. Suppose $N_A = N_{A^2}$. Show that for any integer $m > 0$ we have $N_{A^m} = N_A$. (Here $N$ denotes the null space.)
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $x \in N_A$, then it is obvious that $A^m x = 0$ for any $m > 0$. Thus, $N_A \subset N_{A^m}$.
    
    \item Suppose $x \in N_{A^m}$. 
    
    If $m$ is even, then there exists $n \in \mathbb{N}$ such that $2n = m$. Then, 
    $$\underbrace{(A^2) \cdots (A^2)}_{n} x = 0.$$ 
    Since $N_A = N_{A^2}$, then 
    $$A \underbrace{(A^2) \cdots (A^2)}_{n - 1} x = \underbrace{(A^2) \cdots (A^2)}_{n-1} A x = 0.$$ 
    Continue this process and we can have $A^2x = 0$. Thus, $x \in N_A$ and hence $N_{A^m} \subset N_A$.
    
    If $m$ is odd, then there exists $n \in \mathbb{N}$ such that $2n + 1 = m$. Similarly, we have 
    $$\underbrace{(A^2) \cdots (A^2)}_{n} A x = 0,$$ 
    and continuing the similar process as before also gives $A^2x = 0$. Thus, $N_{A^m} \subset N_A$.
\end{enumerate}
Thus, we have $N_{A^m} = N_A$.
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{August_2016_2}]
We can also prove this by induction.

For $k = 1$, the statement is true. Now suppose the statement is true for $k = m$. For any $x \in N_{A^{m+1}}$, we have $A^{m+1}x = 0$, which is equivalent to $A^m Ax = 0$. Since $N_{A^m} = N_A$, then $Ax \in N_A$, and hence $A^2x = 0$. By $N_A = N_{A^2}$, we have $x \in N_A$. Thus, $N_{A^{m+1}} \subset N_A$.

On the other hand, for any $x \in N_A$, we have $A^{m+1}x = A^m Ax = 0$. Thus, $N_A \subset N_{A^{m+1}}$.

Now we have $N_{A^{m+1}} = N_A$ and the statement is true for $k = m + 1$.
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $\{v_1, v_2, v_3\} \subset \mathbb{Z}^3$ be vectors with integer coordinates. Show that every vector in $\mathbb{Z}^3$ is a linear combination of the $v_i$ with integer coefficients if and only if the (Euclidean $3$-dimensional) volume of the parallelpiped $P$ they form is equal to $1$. The parallelpiped $P$ form by the $v_i$ is the set 
\begin{align*}
    P = \{c_1 v_1 + c_2 v_2 + c_3 v_3 \, | \, 0 \leq c_i \leq 1, \forall i = 1,2,3\}.
\end{align*}
\end{exercise}
\begin{proof}
Let $v_1 = (x_1, y_1, z_1), v_2 = (x_2, y_2, z_2), v_3 = (x_3, y_3, z_3)$, and $x_i, y_i, z_i$ are integers. The volume of $P$ is the absolute value of determinant of the matrix $D$, where $D$ is defined by
\begin{align*}
    D = \begin{pmatrix}
        x_1 & x_2 & x_3 \\
        y_1 & y_2 & y_3 \\
        z_1 & z_2 & z_3
    \end{pmatrix}.
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item $\Rightarrow$) If every vector in $\mathbb{Z}^3$ is a linear combination of the $v_i$, then $\{v_1, v_2, v_3\}$ forms a basis for $\mathbb{Z}^3$ with integer coefficients, since $\dim \mathbb{Z}^3 = 3$. Then, for standard basis $\{e_1, e_2, e_3\}$, there exists unique $a_{ij} \in \mathbb{Z}$ such that $e_i = \sum^3_{j=1}a_{ij} v_j$. Let $A = (a_{ij})_{3 \times 3}$, and we have
    \begin{align*}
        AD = \begin{pmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33}
    \end{pmatrix}\begin{pmatrix}
        x_1 & x_2 & x_3 \\
        y_1 & y_2 & y_3 \\
        z_1 & z_2 & z_3
    \end{pmatrix} = \begin{pmatrix}
        e_1 \\
        e_2 \\
        e_3
    \end{pmatrix} = I.
    \end{align*}
    Then, we have $\det A \det D = 1$. And since $a_{ij}, x_i, y_i, z_i$ are integers, then $\det D = \pm 1$. Thus, The volume of $P$ is $1$.
    
    \item $\Leftarrow$) If the volume of $P$ is $1$, then $\det D = \pm 1$ and hence $\{v_1, v_2, v_3\}$ is linearly independent. Then for any vector $x = (a_1, b_1, c_1) \in \mathbb{Z}^3$, there exists $l_1, l_2, l_3 \in \mathbb{R}$ such that $x = l_1 v_1 + l_2 v_2 + l_3 v_3$, that is 
    \begin{align*}
        l_1 x_1 + l_2 x_2 + l_3 x_3 & = a_1, \\
        l_1 y_1 + l_2 y_2 + l_3 y_3 & = b_1, \\
        l_1 z_1 + l_2 z_2 + l_3 z_3 & = c_1.
    \end{align*}
    By Cramer's rule, we have
    \begin{align*}
        l_1 = \frac{\det \begin{pmatrix} 
        a_1 & x_2 & x_3 \\
        b_1 & y_2 & y_3 \\
        c_1 & z_2 & z_3
        \end{pmatrix}}{\det D},
    \end{align*}
    and since $a_1, b_1, c_1, x_i, y_i, z_i$ are all integers, then $l_1 \in \mathbb{Z}$, so are $l_2, l_3$. Thus, every vector in $\mathbb{Z}^3$ is a linear combination of the $v_i$ with integer coefficients.
    \end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Prove the set of diagonalizable matrices is dense in the set of all complex $n \times n$ matrices, that is, any matrix is a limit of a sequence of diagonalizable matrices.
\end{exercise}
\begin{proof}
For any $n \times n$ matrix $A$, there exists a unitary matrix $U$ such that $U^*AU = T$ is an upper triangular matrix. Let $T_1$ be a matrix defined by
\begin{align*}
    T_1 = T + \begin{pmatrix} 
        \varepsilon_1 & & & \\
        & \frac{\varepsilon_1}{2} & & \\
        & & \ddots & \\
        & & & \frac{\varepsilon_1}{n}
    \end{pmatrix},
\end{align*}
such that the diagonal entries of $T_1$ are distinct, then $T_1$ is diagonalizable. There also exists $0 < \varepsilon_2 < \varepsilon_1$ such that $t_{ii} + \varepsilon/n$ are distinct, then $T_2 = T + \operatorname{diag}[\varepsilon_2, \varepsilon_2/2, \cdots, \varepsilon_2/n]$ is also diagonalizable. Continue this process and we have a sequence of diagonalizable matrices $\{T_k\}$, with $\lim_{k\to\infty}\varepsilon_k = 0$.

Now we have 
\begin{align*}
    \left\|A - U T_k U^*\right\|^2 & = \sup_{x \in \mathbb{C}^n, \|x\| = 1} \left\|(A - U T_k U^*)x\right\|^2 \\
    & = \sup \left\|U(T - T_k)U^*x\right\| \\
    & = \sup \left(U(T - T_k)U^*x, U(T - T_k)U^*x \right) \\
    & = \sup \left((T - T_k)U^*x, U^*U(T - T_k)U^*x \right) \\
    & = \sup \left((T - T_k)U^*x, (T - T_k)U^*x \right)\\
    & = \sup \sum^n_{i=1} \frac{\varepsilon_k^2}{i^2} |x_i|^2 \to 0,
\end{align*}
where $U^* x = (x_1, \cdots, x_n)$. Thus, any matrix $A$ is a limit of a sequence of diagonalizable matrices.
\end{proof}

\medskip

\begin{exercise}
Suppose $A = (a_{ij})$ is an $n \times n$ positive definite matrix over $\mathbb{C}$. Show that the determinant of any principal submatrix of $A$ is a positive real number. (A principal submatrix is a submatrix of $A$ whose entries are $a_{pq}$ for $p, q \in I$, where $I = \{i_1, \cdots, i_k\}$ is some subset of $\{1, \cdots, n\}$.)
\end{exercise}
\begin{proof}
Let $B$ be a principal submatrix of $A$ whose entries are $a_{pq}$ for $p, q \in I$, where $I = \{i_1, \cdots, i_k\}$ is some subset of $\{1, \cdots, n\}$. Let $x = (x_1, \cdots, x_n) \in \mathbb{C}^n$ and $y = (x_{i_1}, \cdots, x_{i_k}) \in \mathbb{C}^k$, also let $x_j = 0$ if $j \notin I$. Then,
\begin{align*}
    (y, By) = (x, Ax) > 0,
\end{align*}
which implies that $B$ is also a positive definite matrix. Thus, $\det B > 0$.
\end{proof}

\medskip

\begin{exercise}
Let $M$ be an $n \times n$ complex matrix. The nullity sequence of $M$ is the sequence $\{N_1(M), N_2(M), \cdots\}$, where $N_k(M) = \operatorname{null} (M^k) = \dim N_{M^k}$ for every $k$. Let $A, B$ be $n \times n$ complex nilpotent matrices. Show that $A$ and $B$ are similar if and only if they have the same nullity sequence.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item ($\Rightarrow$) If $A$ and $B$ are similar, then there exists an invertible matrix $P$ such that $A = PBP^{-1}$. Since $R_A = R_B$, we have
    \begin{align*}
        \rank \left(A^k\right) = \rank \left(PBP^{-1}\right)^k = \rank \left(PB^kP^{-1}\right) = \rank \left(B^k\right).
    \end{align*}
    By rank-nullity theorem, we have $N_k(A) = N_k(B)$ and thus, $A$ and $B$ have the same nullity sequence.
    
    \item ($\Leftarrow$) Suppose $A$ and $B$ have the same nullity sequence. Consider Jodran forms $J_A$ and $J_B$ of $A, B$ in the form
    \begin{align*}
        J_A = \begin{pmatrix}
            J_1 & & \\
            & \ddots & \\
            & & J_m
        \end{pmatrix}, \quad J_B = \begin{pmatrix}
            K_1 & & \\
            & \ddots & \\
            & & K_l
        \end{pmatrix},
    \end{align*}
    where $J_j$ and $K_j$ have form 
    \begin{align*}
        \begin{pmatrix}
            0 & 1 & & & \\
            & 0 & \ddots & & \\
            & & \ddots & \ddots & \\
            & & & 0 & 1 \\
            & & & & 0
        \end{pmatrix}.
    \end{align*}
    Also, $J_j^2$ and $K_j^2$ have form
    \begin{align*}
        \begin{pmatrix}
            0 & 0 & 1 & & & \\
            & 0 & 0 & & & \\
            & & 0 & \ddots & & \\
            & & & \ddots & \ddots & 1 \\
            & & & & \ddots & 0 \\
            & & & & & 0
        \end{pmatrix},
    \end{align*}
    which implies $\dim N_{J_j^k}$ and $\dim N_{K_j^k}$ increase $1$ as $k$ increase $1$. Since $A$ and $B$ have the same nullity sequence, then $\dim N_{J_j^k} = \dim N_{K_j^k}$ for all $k$, and hence $J_A$ and $J_B$ has same number of Jordan blocks, that is $m = l$. 
    
    Also, we can have that for each $i$, $J_i = K_i$. Otherwise, for some $\tilde{i}$, $J_{\tilde{i}} \neq K_{\tilde{i}}$. Without losing generality, assume that $J_{\tilde{i}}$ is a $r \times r$ Jordan block and $K_{\tilde{i}}$ is a $s \times s$ Jordan block, and $r < s$. Then, $J_{\tilde{i}}^{r-1} = 0 \neq K_{\tilde{i}}^{r-1}$, which is a contradiction. Therefore, for each $i$, $J_i = K_i$, that is $A$ and $B$ have the same Jordan form. Thus, $A$ and $B$ are similar.
\end{enumerate}
\end{proof}


\newpage
\section{May 2016 Exam}

\begin{exercise}\label{May_2016_1}
Let $A$ and $B$ be $n \times n$ complex matrices. Show that $AB$ and $BA$ have the same characteristic polynomial. Is it true that $AB$ and $BA$ also have the same minimal polynomial? (Remember to consider the case when $A$ and $B$ are not invertible.)
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $A$ is invertible and $\lambda$ is an eigenvalue of $AB$, then we have $\det (AB - \lambda I) = 0$. On the other hand,
    \begin{align*}
        \det (AB - \lambda I) & = \det A^{-1} \det (AB - \lambda I) \det A \\
        & = \det (BA - \lambda I),
    \end{align*}
    then $\det (BA - \lambda I) = 0$, which implies $\lambda$ is also an eigenvalue for $BA$. Thus, $AB$ and $BA$ have the same characteristic polynomial.
    
    If $A$ is not invertible, then there exists $\varepsilon_k > 0$ such that $\lim_{k\to\infty} \varepsilon_k = 0$ and $A_k = A + \varepsilon_k I$ is invertible. Then, by the statement before, $A_kB$ and $BA_k$ have the same characteristic polynomial. Since $AB = \lim_{k\to\infty}A_kB$ and $BA = \lim_{k\to\infty}BA_k$, then $AB$ and $BA$ have the same characteristic polynomial.
    
    \item In general, $AB$ and $BA$ does not have the same minimal polynomial. Let
    \begin{align*}
        A = \begin{pmatrix}
            0 & 1 & 0 \\
            0 & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}, \quad B = \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix},
    \end{align*}
    then we have 
    \begin{align*}
        AB = \begin{pmatrix}
            0 & 1 & 0 \\
            0 & 0 & 0 \\
            0 & 0 & 0 \\
        \end{pmatrix}, \quad BA = 0,
    \end{align*}
    and thus $m_{AB}(\lambda) = \lambda^2 \neq m_{BA}(\lambda) = \lambda$.
\end{enumerate}
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{May_2016_1}(a)]
Since
\begin{align*}
    \begin{pmatrix}
        xI & A \\
        B & I
    \end{pmatrix}\begin{pmatrix}
        I & 0 \\
        -B & xI
    \end{pmatrix} & = \begin{pmatrix}
        xI - AB & xA \\
        0 & xI
    \end{pmatrix}, \\
    \begin{pmatrix}
        I & 0 \\
        -B & xI
    \end{pmatrix}\begin{pmatrix}
        xI & A \\
        B & I
    \end{pmatrix} & = \begin{pmatrix}
        xI & A \\
        0 & xI - AB
    \end{pmatrix},
\end{align*}
then we have
\begin{align*}
    \det \begin{pmatrix}
        xI - AB & xA \\
        0 & xI
    \end{pmatrix} = \det \begin{pmatrix}
        xI & A \\
        0 & xI - AB
    \end{pmatrix},
\end{align*}
which implies 
\begin{align*}
    x^n \det (xI - AB) = x^n \det (xI - BA).
\end{align*}
Since $x^n$ is nonzero polynomial, then we have
\begin{align*}
    \det (xI - AB) = \det (xI - BA),
\end{align*}
and thus $AB$ and $BA$ have the same characteristic polynomial.
\end{proof}

\medskip

\begin{exercise}
Let $A, B$ be $n \times n$ real matrices. Suppose $A + B = I$ and $\operatorname{rank}(A) + \operatorname{rank}(B) = n$, show that $R_A \cap R_B = \{0\}$.
\end{exercise}
\begin{proof}
For any $x \in \mathbb{R}^n$, if $x \in R_{A + B}$, then there exist $y \in \mathbb{R}^n$ such that $x = (A + B)y \in R_A + R_B$. Therefore, $R_{A + B} \subset R_A + R_B$, which implies $\dim R_{A + B} \leq \dim R_A + \dim R_B$. Since $A + B = I$, then $\dim R_{A + B} = n$. Now we have $\operatorname{rank}(A) + \operatorname{rank}(B) = n$, then
\begin{align*}
    \dim (R_A \cap R_B) & = \dim R_A + \dim R_B - \dim (R_A + R_B) \\
    & \leq \dim R_A + \dim R_B - \dim R_{A + B} \\
    & \leq n - n = 0.
\end{align*}
Thus, $\dim (R_A \cap R_B) = 0$ and hence $R_A \cap R_B = \{0\}$.
\end{proof}

\medskip

\begin{exercise}{\rm *}
Prove that any unitary matrix $Q$ has a square root which is also unitary, i.e. there is a unitary matrix $R$ such that $R^2 = Q$.
\end{exercise}
\begin{proof}
Since $QQ^* = Q^*Q = I$, then $Q$ is normal and hence it can be unitarly diagonalized, that is there exists a unitary matrix $U$ such that $Q = U \Lambda U^*$, where $\Lambda$ is diagonal. Also, we have
\begin{align*}
    \Lambda \Lambda^* = U^* Q U (U^* Q U)^* = U^* Q Q^* U = I,
\end{align*}
and then $\Lambda$ has the form
\begin{align*}
    \Lambda = \begin{pmatrix}
        e^{i\theta_1} & & \\
        & \ddots & \\
        & & e^{i\theta_n}
    \end{pmatrix}.
\end{align*}
Let $P = \begin{pmatrix} e^{i\theta_1/2} & & \\ & \ddots & \\ & & e^{i\theta_n/2} \end{pmatrix}$, then $P^2 = \Lambda$. Let $R = U P U^*$, then we have 
$$R^2 = U P U^* U P U^* = U P P U^* = Q.$$ 
Also, we have
\begin{align*}
    RR^* = U P U^*(U P U^*)^* = U P P^* U^* = I,
\end{align*}
and thus $R$ is unitary.
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $v \in \mathbb{C}^n$ be a nonzero vector (written as a clomun vector) and let $M$ be the $n \times n$ matrix defined by $M = v v^T$.
\begin{enumerate}[label=(\alph*)]
    \item Find the eigenvalues, eigenvectors, characteristic polynomial and minimal polynomial of $M$. 
    
    \item Is $M$ diagonalizable?
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let $v = (x_1, x_2, \cdots, x_n)^T$, then we have $M$ is symmetric, since
    \begin{align*}
        M = \begin{pmatrix}
            x_1^2 & x_1x_2 & \cdots & x_1x_n \\
            x_1x_2 & x_2^2 & \cdots & x_2x_n \\
            \vdots & \vdots & \ddots & \vdots \\
            x_1x_n & x_2x_n & \cdots & x_n^2
        \end{pmatrix}.
    \end{align*}
    Also, we have $M^2 = cM$, where $c = \sum^n_{i=1} x_i^2$. Then, $\lambda^2 - c\lambda$ is a polynomial of $M$ that annihilates $M$. Since $M \neq 0$, then $\lambda$ is not the minimal polynomial of $M$. If $M - cI = 0$, then $x_i^2 - \sum^n_{j=1}x_j^2 = 0$ for each $i$, which implies $x_i = 0$ for each $i$. This is a contradiction with the fact that $v \neq 0$. Thus, the minila polynomial of $M$ is $m_M(\lambda) = \lambda^2 - c \lambda$.
    
    For eigenvalue $0$, let $y = (y_1, \cdots, y_n)$ be eigenvector of $M$. Then, we have $My = 0$, which implies $x_i \sum^n_{j=1} x_j y_i = 0$ for each $i$. Since $v \neq 0$, then at least one of $x_i$ is nonzero, and we can assume $x_n \neq 0$. Then, we can have eigenvectors
    \begin{align*}
        y = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \\ - \frac{x_1}{x_n} \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0 \\ - \frac{x_2}{x_n} \end{pmatrix}, \cdots, \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1 \\ - \frac{x_{n-1}}{x_n} \end{pmatrix},
    \end{align*}
    and hence the dimension of eigenspace for eigenvalue $0$ is $n - 1$.
    
    For eigenvalue $c$, if $c = 0$, then the characteristic polynomial of $M$ is $p_M(\lambda) = \lambda^n$ and the eigenvectors are shown above. If $c \neq 0$, then the characteristic polynomial of $M$ is $p_M(\lambda) = \lambda^{n - 1}(\lambda - c)$, then the eigenvectors for $\lambda = 0$ is shown above and the eigenvectors for $\lambda = c$ is $v$.
    
    \item If $c = 0$, then the dimension of eigenspace for eigenvalue $0$ is $n - 1$, then $M$ is not diagonalizable.
    
    If $c \neq 0$, then $M$ is diagonalizable, since $p_M(\lambda) = \lambda^{n - 1}(\lambda - c)$ and $\dim N_{M - cI} = n - 1$, $\dim N_{M} = 1$.
\end{enumerate}
\end{proof}

\medskip

\newpage
\begin{exercise}
Let $A$ be an $n \times n$ complex matrix such that the minimal polynomial $m_A$ is $m_A(t) = t^n$. Prove that there is a vector $v \in \mathbb{C}^n$ such that $\left\{v, Av, \cdots, A^{n-1}v\right\}$ is a basis for $\mathbb{C}^n$.
\end{exercise}
\begin{proof}
Since the minimal polynomial $m_A$ is $m_A(t) = t^n$, then $A^{n-1} \neq 0$. Then there exists a vector $v$ such that $A^{n-1}v \neq 0$. Suppose there exists $c_0, c_1, \cdots, c_{n-1}$ such that
\begin{align*}
    c_0 v + c_1 Av + c_2 A^2v + \cdots + c_{n-1} A^{n-1}v = 0.
\end{align*}
Applying $A^{n-1}$ on both sides gives $c_0 A^{n-1}v = 0$, that is $c_0 = 0$. Then, applying $A^{n-2}$ on both sides gives $c_1 = 0$. Continue this process and we have $c_0 = c_1 = \cdots = c_{n-1} = 0$, which implies $\left\{v, Av, \cdots, A^{n-1}v\right\}$ is linearly independent. Since there are $n$ vectors in $\left\{v, Av, \cdots, A^{n-1}v\right\}$, then it is a basis for $\mathbb{C}^n$.
\end{proof}

\medskip

\begin{exercise}{\rm *}
~\begin{enumerate}[label=(\alph*)]
    \item Let $A$ and $B$ be $n \times n$ complex matrices which are non-negative (i.e. self-adjoint and all the eigenvalues are nonnegative). Show that
    \begin{align*}
        \det (A + B) \geq \det (A) + \det (B).
    \end{align*}
    
    \item Show that the set of positive (definite) matrices with determinant greater than $1$ is a convex set. Recall that a set $S$ is convex if for any $A, B \in S$ and $0 \leq t \leq 1$, we have $tA + (1-t)B \in S$. {\bf Hint:} log-concavity.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A, B$ are non-negative, then $A + B$ is also non-negative. Indeed, for any $x \in \mathbb{C}^n$, $(x, (A+B)x) = (x, Ax) + (x, Bx) \geq 0$.
    
    If $A$ and $B$ are both not invertible, then $\det(A+B) \geq \det(A) + \det(B) = 0$.
    
    If one of $A$ and $B$ is invertible, saying $A$, then we have
    \begin{align*}
        \det(A + B) & = \det \left(A^{-1} (A + B) A \right) \\
        & = \det \left(A + A^{-1} BA \right) \\
        & = \det (A) \det \left(I + A^{-1} B\right).
    \end{align*}
    Also, since $A$ is invertible, then we have
    \begin{align*}
        A^{-1} B = A^{-\frac{1}{2}} A^{-\frac{1}{2}} B A^{-\frac{1}{2}} A^{\frac{1}{2}},
    \end{align*}
    that is $A^{-1} B$ is similar to $A^{-\frac{1}{2}} B A^{-\frac{1}{2}}$, which is also non-negative. Indeed, for any $x \in \mathbb{C}^n$, 
    \begin{align*}
        \left(x, A^{-\frac{1}{2}} B A^{-\frac{1}{2}} x\right) = \left(A^{-\frac{1}{2}} x, B A^{-\frac{1}{2}} x\right) \geq 0.
    \end{align*}
    Then, $A^{-1} B$ is also non-negative. Therefore, $A^{-1} B$ is diagonalizable, that is, there exists a unitary matrix $P$ such that $A^{-1}B = P \Lambda P^*$, where $\Lambda$ is diagonal.
    
    Let $\lambda_1, \cdots, \lambda_n$ be eigenvalues of $A^{-1} B$, then 
    \begin{align*}
        \det \left(I + A^{-1} B\right) & = \det \left(P^* \left(I + A^{-1} B\right) P \right) \\
        & = \det \left(I + \Lambda \right) \\
        & = \prod^n_{i=1} (1 + \lambda_i) \\
        & \geq 1 + \prod^n_{i=1} \lambda_i = I + \det \left(A^{-1} B\right).
    \end{align*}
    Thus, we have
    \begin{align*}
        \det(A + B) & = \det (A) \det \left(I + A^{-1} B\right) \\
        & \geq \det (A) \left(I + \det \left(A^{-1} B\right) \right) \\
        & = \det(A) + \det(B).
    \end{align*}
    
    \item By log-concavity, we have
    \begin{align*}
        \det (tA + (1-t)B) \geq (\det A)^t (\det B)^{1-t} > 1,
    \end{align*}
    thus, $S$ is convex.
\end{enumerate}
\end{proof}

\medskip

\begin{proof}[Simple Proof of log-concavity]
Since $B$ is invertible, then
\begin{align*}
    \det(tA + (1-t)B) = \det(B) \det\left(tB^{-1}A + (1-t)I\right).
\end{align*}
Suppose $\lambda_1, \cdots, \lambda_n$ are eigenvalues of $B^{-1}A$. Now we condier function $f(x) = tx + (1-t) - x^t$, then 
\begin{align*}
    f'(x) = t - tx^{t-1},
\end{align*}
and $f(x)$ is decreasing for $x \in (0,1)$ and $f(x)$ is increasing for $x > 1$. Therefore, $f(x) \geq f(1) = 0$. Then, we have
\begin{align*}
    \det\left(tB^{-1}A + (1-t)I\right) & = \prod^n_{i=1} (t \lambda_i + 1 - t) \\
    & \geq \prod^n_{i=1}\lambda_i^t = \left(\det(B^{-1}A)\right)^t.
\end{align*}
Thus, 
\begin{align*}
    \det(tA + (1-t)B) & = \det(B) \det\left(tB^{-1}A + (1-t)I\right)\\
    & \geq \det(B) \left(\det(B^{-1}A)\right)^t \\
    & = \det(B) \det\left(B^{-1}\right) (\det A)^t \\
    & = (\det A)^t (\det B)^{1-t}.
\end{align*}
\end{proof}


\newpage
\section{August 2015 Exam}

\begin{exercise}
Let $V$ be a finite dimensional vector space. Prove that every linearly independent set of $V$ can be extended to a basis for $V$.
\end{exercise}
\begin{proof}
Let $\dim V = n < \infty$. Let $S$ be any linearly independent set in $V$, and then $S$ cannot contain more than $n$ vectors, since any $m$ vectors in $V$, $m > n$ are linearly dependent. Suppose $S = \{s_1, s_2, \cdots, s_k \}$, $k \leq n$. 

If $\operatorname{span}(S) = V$, then $S$ is a basis for $V$.
If $\operatorname{span}(S) \neq V$, then there exists a vector $v_1 \in V \setminus \operatorname{span}(S)$. Then the set $S_1 = S \cap \{v_1\}$ is linearly independent. If If $\operatorname{span}(S_1) = V$, then $S_1$ is a basis for $V$. If not, then we can continue the process like before. 

After continuing this process $n - k$ times, we get a set with $n$ linearly independent vectors,
\begin{align*}
    S_{n - k} = S \cap \{v_1, v_2, \cdots, v_{n-k}\},
\end{align*}
then $\dim S_{n-k} = n$, and hence $\operatorname{span}(S_{n-k}) = V$. Thus, $S_{n-k}$ is a basis for $V$.
\end{proof}

\medskip

\begin{exercise}
Let $A, B, C$ be $n \times n$ matrices satisfying $AB = BA$. Show that
\begin{align*}
    \det (A + BC) = \det (A + CB).
\end{align*}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $B$ is invertible, then by $AB = BA$, we have
    \begin{align*}
        \det (A + BC) & = \det \left(B^{-1}(A + BC)B\right) \\
        & = \det \left(B^{-1}AB + CB\right) \\
        & = \det \left(B^{-1}BA + CB\right) \\
        & = \det \left(A + CB\right).
    \end{align*}
    
    \item If $B$ is not invertible, then there exists $\varepsilon_k > 0$ and $\varepsilon_k \to 0$ such that $B_k = B + \varepsilon_k I$ is invertible. Then, taking $k \to \infty$ we could get the result.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Prove that for any $n \times n$ complex matrix $A$, 
\begin{align*}
    \left|\operatorname{tr}(A^*A) \right| \leq n \|A\|^2,
\end{align*}
where $\|A\|$ is the operator norm of $A$ defined by
\begin{align*}
    \|A\| = \sup_{x \in \mathbb{C}^n, x \neq 0} \frac{|Ax|}{x}.
\end{align*}
\end{exercise}
\begin{proof}
Since $(A^*A)^* = A^*A$, then $A^*A$ is self-adjoint and hence can be diagonalized by a unitary matrix, that is there exists an orthogonal basis $\{v_1, \cdots, v_n\}$ in $\mathbb{C}^{n}$, such that $A^*A v_i = \lambda_i v_i$, where $\lambda_i$ is $i$-th eigenvalue of $A^*A$. Then, we have
\begin{align*}
    \left|\operatorname{tr}(A^*A) \right| & = \left|\sum^n_{i=1} \lambda_i \right| \leq \sum^n_{i=1} \left|\lambda_i \right| \\
    & = \sum^n_{i=1} \left|(\lambda_i v_i, v_i) \right| \\
    & = \sum^n_{i=1} \left|(A^*A v_i, v_i) \right| \\
    & = \sum^n_{i=1} \left|Av_i \right|^2 \\
    & \leq \sum^n_{i=1} \|A\|^2 = n \|A\|^2.
\end{align*}
\end{proof}

\medskip

\begin{exercise}
Let $P_4$ be the vector space of polynomials with real coefficients of orders at most $3$. Consider the linear map
\begin{align*}
    T: p(t) \mapsto p(t + 1) - p(t).
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item Find all eigenvalues of corresponding eigenspaces of $T$.
    
    \item Find the Jordan canonical form of $T$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For any $p(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3$, $T(p) = a_3 t^2 + (3a_3 + 2a_2)t + (a_1 + a_2 + a_3)$. Then, under the basis $\mathcal{B} = \{1, t, t^2, t^3\}$, 
    \begin{align*}
        [T]_{\mathcal{B}} = \begin{pmatrix}
            0 & 1 & 1 & 1 \\
            0 & 0 & 2 & 3 \\
            0 & 0 & 0 & 3 \\
            0 & 0 & 0 & 0
        \end{pmatrix},
    \end{align*}
    then the charasteristic polynomial of $T$ is $p_T(\lambda) = \lambda^4$, and the only eigenvalue is $\lambda = 0$. Then, the eigenvector is $(1, 0, 0, 0)^T$, and hence the eigenspace of $T$ is $\{c\, | \, c \in \mathbb{R}\}$.
    
    \item Since $\dim N_{T} = 1$, then there is only one Jordan block in its Jordan form. Thus, the Jordan canonical form of $T$ is
    \begin{align*}
        J_T = \begin{pmatrix}
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1 \\
            0 & 0 & 0 & 0
        \end{pmatrix}.
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $A(t)$ be a differentiable $n \times n$ matrix valued function. Is it always true that 
\begin{align*}
    \dv[]{}{t} A^2(t) = 2 A(t) \dv[]{A(t)}{t}?
\end{align*}
Prove it or provide a counter example.
\end{exercise}
\begin{proof}
For $n = 1$, and $A(t)$ is a function of one variable, then it is true that $\dv[]{}{t} A^2(t) = 2 A(t) \dv[]{A(t)}{t}$.

For $n \geq 2$, the statement is not always true. For example, let $A(t) = \begin{pmatrix} 0 & \sin x \\ \cos x & 0 \end{pmatrix}$, then
\begin{align*}
    \dv[]{}{t} A^2(t) & = \begin{pmatrix} \cos^2 x - \sin^2 x & 0 \\ 0 & \cos^2 x - \sin^2 x \end{pmatrix}, \\
    2 A(t) \dv[]{A(t)}{t} & = \begin{pmatrix} -2\sin^2 x & 0 \\ 0 & 2\cos^2 x \end{pmatrix},
\end{align*}
and thus the equality does not hold for $A(t)$.
\end{proof}

\medskip

\begin{exercise}
Prove that if $A$ is an invertible $n \times n$ matrix with integer entries, then $A^{-1}$ has integer entries if and only if $\det(A) = \pm 1$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item ($\Rightarrow$) Since $\det (A) \det (A^{-1}) = 1$ and $\det (A), \det (A^{-1}) \in \mathbb{Z}$, then $\det(A) = \pm 1$.
    
    \item ($\Leftarrow$) If $\det(A) = \pm 1$, by $A^{-1} = (\det A)^{-1} \operatorname{adj}(A)$, we could have $^{-1} = \pm \operatorname{adj}(A)$. And since $\operatorname{adj}(A)$ is also a matrix with integer entries, then so is $A^{-1}$.
\end{enumerate}
\end{proof}


\newpage
\section{April 2015 Exam}

\begin{exercise}
Let $\Pi$ be the plane in $\mathbb{R}^3$ defined by
\begin{align*}
    3x + 2y + z = 0.
\end{align*}
Let $T$ be the reflection in $\mathbb{R}^3$ about the plane $\Pi$. Find the matrix representation of $T$.
\end{exercise}
\begin{proof}
For any $x \in \mathbb{R}^3$, its reflection about the plane $\Pi$ is $Tx = x - 2 \frac{(x, u)}{(u,u)}u$, where $u$ is the vector that is orthogonal to $\Pi$ and $u = (3, 2, 1)^T$. Thus, we have
\begin{align*}
    T = I - 2 \frac{uu^T}{(u,u)} = \frac{1}{14} \begin{pmatrix}
        -4 & -12 & -6 \\
        -12 & 6 & -4 \\
        -6 & -4 & 12
    \end{pmatrix}.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Suppose $A \neq 0$ is a nilpotent complex matrix. Prove that $A + A^*$ is not nilpotent.
\end{exercise}
\begin{proof}
If $A$ is diagonalizable, then there exists invertible matrix $P$ such that $A = PDP^{-1}$, where $D$ is diagonal. Also, there exists $k \in \mathbb{N}$ such that $A^k = 0$, and then $PD^kP^{-1} = 0$, which implies $D = 0$. Then, $A = 0$, which is a contradiction. Therefore, nonzero nilpotent matrix is not diagonalizable.

If $A + A^* = 0$, then $A^* = -A$. Then we have $AA^* = -A^2 = A^*A$, hence $A$ is normal. Therefore, $A$ is digaonalizable. Since the only diagonalizable nilpotent matrix is zero matrix, then we have a contradiction.

If $A + A^* \neq 0$, and since $A + A^*$ is self adjoint, then $A + A^*$ is diagonalizable. Thus, $A + A^*$ is not nilpotent.
\end{proof}

\medskip

\begin{exercise}
Let $A$ be a square matrix with the characteristic polynomial $(t + 1)^2 (t - 1)^3 (t^2 + 1)^2$ and the minimal polynomial $(t + 1) (t - 1)^2 (t^2 + 1)$. Find Jordan canonical form of $A$.
\end{exercise}
\begin{proof}
By the characteristic polynomial, $A$ has eigenvalues $\lambda = -1, 1, i, -i$ with multiplicity $2, 3, 2, 2$ respectively. Consider the Jordan block $J_1$ for $\lambda = 1$, it has two choices
\begin{align*}
    J_1 = \begin{pmatrix} 
        -1 & 1\\
        0 & -1
    \end{pmatrix}, \,\, {\rm or}\,\, J_1 =  \begin{pmatrix} 
        -1 & 0\\
        0 & -1
    \end{pmatrix},
\end{align*}
by the minimal polynomial of $A$, we could have that the first one does not satisfy $J_1 + I = 0$. Then we have $J_1 = \begin{pmatrix} -1 & 0 \\ 0 & -1 \end{pmatrix}$. Similarly, we have Jordan canonical form of $A$
\begin{align*}
    J_A = \begin{pmatrix} 
    -1 & 0 & & & & & & & \\
    & -1 & 0 & & & & & & \\
    & & 1 & 1 & & & & & \\
    & & & 1 & 0 & & & & \\
    & & & & 1 & 0 & & & \\
    & & & & & i & 0 & & \\
    & & & & & & -i & 0 & \\
    & & & & & & & i & 0 \\
    & & & & & & & & -i
    \end{pmatrix}.
\end{align*}
\end{proof}

\medskip

\begin{exercise}
Consider a ${\bf flag}$ of linear subspaces ${\bf F} = \left(\{0\} \subsetneqq F_1 \subsetneqq \cdots \subsetneqq F_n = \mathbb{C}^n \right)$. Let $T:\mathbb{C}^n \to \mathbb{C}^n$ be a linear transformation such that for every $i = 1, \cdots, n$, $T(F_i) = F_i$. (\textbf{Hint:} Note that $\dim F_k = k$ for $1 \leq k \leq n$.)
\begin{enumerate}[label=(\alph*)]
    \item Show that there is basis $B = \{b_1, \cdots, b_n\}$ for $\mathbb{C}^n$ with the following property: For each $k = 1, \cdots, n$, the set $\{b_1, \cdots, b_k\}$ is a basis for $F_k$.
    
    \item Show that $T$ is represented by an upper-triangular matrix in this basis.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let $F_0 = \{0\}$, and then $\dim F_0 = 0$. Then, $\textbf{F}$ has $n + 1$ sets in which, for each $k = 0, 1, \cdots, n-1$, $F_k$ is a proper subspace of $F_{k+1}$. Then, we have $\dim F_k = k$ for $1 \leq k \leq n$, otherwise one of them will not be a proper subspace of the one behind it.
    
    For $F_1$, $\dim F_1 = 1$, then there exists vector $b_1 \in F_1$ and $\operatorname{span}\{b_1\} = F_1$. For $F_2$, since $\dim F_2 = 2$, then there exists $b_2 \in F_2$ such that $b_2 \notin F_1$ and $\operatorname{span}\{b_1, b_2\} = F_2$. Continue this process, we can find a basis $\{b_1, \cdots, b_k\}$ for $F_k$, then for $F_{k+1}$, we could find $b_{k+1} \in F_{k+1}$ but $b_{k+1} \notin F_k$ such that $\operatorname{span}\{b_1, \cdots, b_k, b_{k+1}\} = F_{k+1}$. By induction, we could find a basis $B = \{b_1, \cdots, b_n\}$ for $F_n = \mathbb{C}^n$.
    
    \item Suppose $T = (t_{ij})_{n \times n}$ is a $n \times n$ matrix. For $F_1$, since $\dim F_1 = 1$, we can assume $\{b_1 = (y_1, 0, \cdots, 0) \in \mathbb{C}^n\}$ is a basis for $F_1$. Then, $Tb_1 \in F_1$ gives $Tb_1 = k b_1$, and then $t_{j1} = 0, j = 2, 3, \cdots, n$. 
    
    For $F_2$, we could assume $\{b_1, b_2\}$ is a basis for $F_2$, where $b_2 = (0, y_2, 0, \cdots, 0) \in \mathbb{C}^n\}$. Also, $Tb_2 \in F_2$ gives $t_{j2} = 0, j = 3, 3, \cdots, n$.
    
    Continue this process and we have $t_{ij} = 0$ for $i > j$, which implies $T$ is represented by an upper-triangular matrix in this basis.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
For two square matrices, the Lie bracket $[A, B]$ is defined as
\begin{align*}
    [A, B] = AB - BA.
\end{align*}
Let $\mathfrak{sl}(2, \mathbb{C})$ denote the vector space of $2 \times 2$ complex matrices with trace $0$.
\begin{enumerate}[label=(\alph*)]
    \item Show that $\mathfrak{sl}(2, \mathbb{C})$ is a $3$ dimensional complex vector space and is closed under the Lie bracket.
    
    \item Show that for any fixed matrix $M \in \mathfrak{sl}(2, \mathbb{C})$, the map
    \begin{align*}
        {\bf ad}({\bf M}): A \to [M, A]
    \end{align*}
    is linear.
    
    \item Let $E = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}$. Find the eigenvalues and Jordan form of the linear operator ${\bf ad}({\bf E})$. (\textbf{Hint:} ${\bf ad}({\bf E})$ is represented as a $3 \times 3$ matrix.)
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Let 
    \begin{align*}
        E_1 = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \quad E_2 = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, \quad E_3 = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix},
    \end{align*}
    then for any $A = \begin{pmatrix} a & b \\ c & -a \end{pmatrix} \in \mathfrak{sl}(2, \mathbb{C})$, $A = b E_1 + c E_2 + a E_3$. Therefore, $\{E_1, E_2, E_3\}$ forms a basis for $\mathfrak{sl}(2, \mathbb{C})$. Thus, $\dim \mathfrak{sl}(2, \mathbb{C}) = 3$.
    
    Also, $\operatorname{tr}(AB) = \operatorname{tr}(BA)$ for any matrices $A, B$. Then, if $A, B \in \mathfrak{sl}(2, \mathbb{C})$, then it is also true that $\operatorname{tr} [A, B] = 0$, and thus $\mathfrak{sl}(2, \mathbb{C})$ is closed under the Lie bracket.
    
    \item For any $c \in \mathbb{C}$, $A, B \in \mathbb{C}^{2 \times 2}$, we have
    \begin{align*}
        {\bf ad}({\bf M})(A + cB) & = [M, A + cB] \\
        & = MA + cMB - AM - cBM \\
        & = [M, A] + c [M, B] \\
        & = {\bf ad}({\bf M})(A) + c \cdot {\bf ad}({\bf M})(B),
    \end{align*}
    thus the map ${\bf ad}({\bf M})$ is linear. 
    
    \item Since 
    \begin{align*}
        {\bf ad}({\bf E})(E_1) = 0, \quad {\bf ad}({\bf E})(E_2) = E_3, \quad {\bf ad}({\bf E})(E_3) = -2E_1,
    \end{align*}
    then the matrix $P$ of ${\bf ad}({\bf E})$ under basis $\mathcal{B} = \{E_1, E_2, E_3\}$ is 
    \begin{align*}
        [P]_{\mathcal{B}} = \begin{pmatrix}
            0 & 0 & -2 \\
            0 & 0 & 0 \\
            0 & 1 & 0
        \end{pmatrix},
    \end{align*}
    and the eigenvalue is $0$. Since $\dim N_{[P]_{\mathcal{B}}} = 1$, then there is only one Jordan block in Jordan form $J_P$ of $[P]_{\mathcal{B}}$ is
    \begin{align*}
        J_P = \begin{pmatrix}
            0 & 1 & 0 \\
            0 & 0 & 1 \\
            0 & 0 & 0
        \end{pmatrix}.
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $A$ be a $3 \times 3$ Hermitian matrix with eigenvalues $\gamma_1 \leq \gamma_2 \leq \gamma_3$. Let $B$ be the $2 \times 2$ matrix which is the upper left corner entries of $A$. Let $\mu_1 \leq \mu_2$ be the eigenvalues of $B$. Use Min-max theorem about eigenvalues to prove that these eigenvalues satisfy the interlacing inequalities:
\begin{align*}
    \gamma_1 \leq \mu_1 \leq \gamma_2 \leq \mu_2 \leq \gamma_3.
\end{align*}
\end{exercise}
\begin{proof}
First, we have
\begin{align*}
    \mu_2 & = \min_{S \subset \mathbb{C}^2, \dim S = 2} \left\{\max_{x \in S, x \neq 0} \frac{(x, Bx)}{(x,x)} \right\} \\
    & = \max_{x =(x_1, x_2) \in \mathbb{C}^2, x \neq 0} \frac{(x, Bx)}{(x,x)} \\
    & = \max_{y \in \mathbb{C}^3, y = (x_1, x_2, 0)} \frac{(y, Ay)}{(y,y)} \\
    & \leq \max_{y \in \mathbb{C}^3, y \neq 0} \frac{(y, Ay)}{(y,y)} = \gamma_3.
\end{align*}
Then, we have the second inequality 
\begin{align*}
    \gamma_2 & = \min_{S \subset \mathbb{C}^3, \dim S = 2} \left\{\max_{x \in S, x \neq 0} \frac{(x, Ax)}{(x,x)} \right\}\\
    & \leq \max_{x = (x_1, x_2, 0) \in \mathbb{C}^2, x \neq 0} \frac{(x, Ax)}{(x,x)} \\
    & = \max_{y = (x_1, x_2), y \neq 0} \frac{(y, By)}{(y, y)} = \mu_2,
\end{align*}
similarly, we can have 
\begin{align*}
    \mu_1 & = \min_{S \subset \mathbb{C}^2, \dim S = 1} \left\{\max_{x = (x_1, x_2) \in S, x \neq 0} \frac{(x, Bx)}{(x,x)} \right\} \\
    & = 
\end{align*}
\end{proof}









\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}